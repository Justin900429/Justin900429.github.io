{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Content table <a name=\"content\"></a>\n",
    "### 1. [Read images, videos, webcam](#read_image)\n",
    "### 2. [Basic function](#basic)\n",
    "### 3. [Resize and crap](#resize)\n",
    "### 4. [Shapes and Texts](#shape)\n",
    "### 5. [Wrap perspective](#wrap)\n",
    "### 6. [Join images](#join)\n",
    "### 7. [Color dectection](#color)\n",
    "### 8. [Contour / Shape dectection](#contour)\n",
    "### 9. [Object detection](#object)\n",
    "\n",
    "--------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read images, videos, webcam <a name=\"read_image\"></a>\n",
    "\n",
    "[Back to content table](#content)\n",
    "\n",
    "* `cv2.imread(source)`  \n",
    "    source: The path of the image\n",
    "\n",
    "  \n",
    "* `cv2.imshow(name, img)`  \n",
    "    * name: Name of the window  \n",
    "    * img: Image to show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "img = cv2.imread(\"resources/scene.jpg\")\n",
    "\n",
    "\n",
    "cv2.imshow(\"Scene\", img)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "img = cv2.imread(\"resources/food.jpg\")\n",
    "\n",
    "# Add noise to the image to see how blurring works\n",
    "gaussian = np.random.normal(0, 0.3, img.shape) * 255\n",
    "img_noisy = img + gaussian\n",
    "img_noisy = np.clip(img_noisy, 0, 255).astype(np.uint8)\n",
    "\n",
    "# Convert the image to gray scale\n",
    "img_noisy_gray = cv2.cvtColor(img_noisy, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "'''\n",
    "Blur the image\n",
    "'''\n",
    "# Average blur\n",
    "img_average = cv2.blur(img_noisy_gray, (3, 3))\n",
    "\n",
    "# Median blur\n",
    "img_median = cv2.medianBlur(img_noisy_gray, 3)\n",
    "\n",
    "# Gaussian blur\n",
    "img_gaussian = cv2.GaussianBlur(img_noisy_gray, (3, 3), 0)\n",
    "\n",
    "# Stack the image (Will introduce latter, the user-defined function)\n",
    "result = stackImages(0.5, [[img, img_noisy, img_noisy_gray], [img_average, img_median, img_gaussian]])\n",
    "\n",
    "cv2.imshow(\"result\", result)\n",
    "\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)\n",
    "\n",
    "cv2.imwrite(\"result/blur.jpg\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `cv2.ViedoCapture(source)` <- cap  \n",
    "    1. source: The path for video  \n",
    "      If source is 0, then use the default webcame\n",
    "  \n",
    "    \n",
    "* `cap.read()`  \n",
    "   1. Return (state, image)\n",
    "       * state: Check whether the camera is working  \n",
    "       * image: Image read from camera\n",
    "       \n",
    "         \n",
    "* `cap.release()`  \n",
    "    Close the camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "cap = cv2.VideoCapture(\"resources/test.mp4\")\n",
    "\n",
    "'''\n",
    "Video is the sequence of images.\n",
    "We need while loop to show the video\n",
    "'''\n",
    "while True:\n",
    "    success, img = cap.read()\n",
    "    cv2.imshow(\"Video\", img)\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break;\n",
    "        \n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# Use the default webcame\n",
    "cap = cv2.VideoCapture(0)\n",
    "cap.set(3, 640)\n",
    "cap.set(4, 480)\n",
    "cap.set(10, 100)\n",
    "\n",
    "'''\n",
    "Video is the sequence of images.\n",
    "We need while loop to show the video\n",
    "'''\n",
    "while True:\n",
    "    success, img = cap.read()\n",
    "    \n",
    "    # To ensure the camera can successfully read the image\n",
    "    if (not success):\n",
    "        continue\n",
    "        \n",
    "    cv2.imshow(\"Video\", img)\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break;\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do what you want to test!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic function <a name=\"basic\"></a>\n",
    "\n",
    "[Back to content table](#content)\n",
    "\n",
    "\n",
    "* `cv2.cvtColor(img, code)`  \n",
    "    * img: Image going to convert\n",
    "    * code: Type for the conversion. See the [documentation](https://docs.opencv.org/2.4/modules/imgproc/doc/miscellaneous_transformations.html#cvtcolor)\n",
    "      \n",
    "    \n",
    "* `cv2.GaussianBlur(img, kernel_size, sigma_X)`  \n",
    "    * img: Image to be propcessed\n",
    "    * kernel_size: Size for the filter matrix\n",
    "    * sigma: Standard deviation of the direction of X  \n",
    "    [Here](https://datacarpentry.org/image-processing/06-blurring/) is how GaussianBlur works!\n",
    "    \n",
    "      \n",
    "* `cv2.canny(img, threshold1, threshold2)`  \n",
    "    * img: Image to be processed  \n",
    "    * threshold1: First threshold  \n",
    "    * threshold2: Second threshold.  \n",
    "    [Here](https://towardsdatascience.com/canny-edge-detection-step-by-step-in-python-computer-vision-b49c3a2d8123) is how Canny works!\n",
    "      \n",
    "   \n",
    "   [Here](https://www.mathworks.com/help/images/morphological-dilation-and-erosion.html) is how both dilation and erosion work!\n",
    "* `cv2.dilate(img, kernel, iteration)`  \n",
    "    The value of the output pixel is the **maximum** value of all pixels in the neighborhood.  \n",
    "    * img: Image to be processed  \n",
    "    * kernel: The size of checking kernel  \n",
    "    * iteration: Times for doing the filter  \n",
    "    \n",
    "      \n",
    "* `cv2.eroded(img, kernel, iteration)`  \n",
    "    The value of the output pixel is the **minimum** value of all pixels in the neighborhood.  \n",
    "    * img: Image to be processed  \n",
    "    * kernel: The size of checking kernel  \n",
    "    * iteration: Times for doing the filter  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "img = cv2.imread(\"resources/car.jpg\")\n",
    "\n",
    "# Change the image to gray scale\n",
    "img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Blur the image to reduce noise\n",
    "img_blur = cv2.GaussianBlur(img_gray, (3, 3), 0)\n",
    "\n",
    "'''\n",
    "# Edge detection\n",
    "'''\n",
    "# Canny detector\n",
    "img_canny = cv2.Canny(img_blur, 100, 100)\n",
    "\n",
    "# LoG detector (Use img, not img_blur)\n",
    "img_log = cv2.Laplacian(img_gray, -1, cv2.CV_16S)\n",
    "\n",
    "# Stack the image\n",
    "result = stackImages(0.5, [img, img_canny, img_log])\n",
    "\n",
    "cv2.imshow(\"Result\", result)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)\n",
    "\n",
    "cv2.imwrite(\"Edge_total.jpg\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "img = cv2.imread(\"resources/shapes.jpg\")\n",
    "\n",
    "# Change the image to gray scale\n",
    "img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Blur the image\n",
    "img_blur = cv2.GaussianBlur(img_gray, (7, 7), 0)\n",
    "\n",
    "# Edge detection\n",
    "img_thresh = cv2.threshold(img_blur, 200, 1, cv2.THRESH_BINARY_INV)[1].astype(np.float)\n",
    "\n",
    "# Define the kernel\n",
    "kernel = np.ones((69, 69), np.uint8)\n",
    "\n",
    "# First erase the noise\n",
    "img_eroded = cv2.erode(img_thresh, kernel, iterations=1)\n",
    "\n",
    "# Back to the original feature\n",
    "img_dilation = cv2.dilate(img_eroded, kernel, iterations=1)\n",
    "\n",
    "# Show the above image\n",
    "result = np.hstack([img_thresh, img_eroded, img_dilation])\n",
    "\n",
    "cv2.imshow(\"Result\", result)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "img = cv2.imread(\"resources/beauty.jpg\")\n",
    "\n",
    "# Change the image to gray scale\n",
    "img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Blur the image\n",
    "img_blur = cv2.GaussianBlur(img_gray, (7, 7), 0)\n",
    "\n",
    "# Edge detection\n",
    "img_canny = cv2.Canny(img, 100, 100)\n",
    "\n",
    "# Define the kernel\n",
    "kernel = np.ones((5, 5), np.uint8)\n",
    "\n",
    "# Need gray scale\n",
    "img_dilation = cv2.dilate(img_canny, kernel, iterations=1)\n",
    "\n",
    "# Need gray scale\n",
    "img_eroded = cv2.erode(img_dilation, kernel, iterations=1)\n",
    "\n",
    "\n",
    "# Show the above image\n",
    "cv2.imshow(\"Gray image\", img_gray)\n",
    "cv2.imshow(\"Blur image\", img_blur)\n",
    "cv2.imshow(\"Canny image\", img_canny)\n",
    "cv2.imshow(\"Dilation image\", img_dilation)\n",
    "cv2.imshow(\"Erosion image\", img_eroded)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do what you want to test!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resize and crap <a name=\"resize\"></a>\n",
    "\n",
    "[Back to content table](#content)\n",
    "\n",
    "* `cv2.resize(img, (width, height))`  \n",
    "    * img: Image to be processed\n",
    "    * (width, height): The size of the new image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "img = cv2.imread(\"resources/food.jpg\")\n",
    "\n",
    "# Print out the size of the image\n",
    "print(img.shape)\n",
    "\n",
    "'''\n",
    "# Resize the image\n",
    "size: (width, heigth)\n",
    "width: 500\n",
    "height: 200\n",
    "'''\n",
    "img_resize = cv2.resize(img, (500, 200))\n",
    "\n",
    "# Print out the new size of the image\n",
    "print(img_resize.shape)\n",
    "\n",
    "\n",
    "'''\n",
    "# Crop the image\n",
    "target region: (height, width)\n",
    "height pixel: 300_700\n",
    "width pixel: 400_100\n",
    "'''\n",
    "img_cropped = img[300:900, 400:1000]\n",
    "\n",
    "# Show the image\n",
    "cv2.imshow(\"Image\", img)\n",
    "cv2.imshow(\"Resize image\", img_resize)\n",
    "cv2.imshow(\"Cropped image\", img_cropped)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "img = cv2.imread(\"resources/food.jpg\")\n",
    "\n",
    "'''\n",
    "# Crop the image\n",
    "target region: (height, width)\n",
    "height pixel: 500~900\n",
    "width pixel: 300~1000\n",
    "'''\n",
    "img_cropped = img[500:900, 300:1000]\n",
    "\n",
    "# Show the image\n",
    "cv2.imshow(\"Image\", img)\n",
    "cv2.imshow(\"Cropped image\", img_cropped)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)\n",
    "\n",
    "cv2.imwrite(\"result/crop.jpg\", img_cropped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do what you want to test!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shapes and Texts <a name=\"shape\"></a>\n",
    "\n",
    "[Back to content table](#content)\n",
    "\n",
    "* `cv2.line(img, start_point, end_point, color, thickness)`  \n",
    "    * img: Image to add line  \n",
    "    * start_point: Pixel to start line  \n",
    "    * end_point: Pixel to end line  \n",
    "    **Both start_point and end_point should be used in (width_pixel, heigth_pixel).** \n",
    "    \n",
    "    * color: Color of line  \n",
    "    * thickness: The thickness of line\n",
    "      \n",
    "    \n",
    "* `cv2.rectangle(img, upper_left, lower_right, color, thickness))`  \n",
    "    * img: Image to add rectangle. \n",
    "    * upper_left: Pixel to start rectangle  \n",
    "    * lower_right: Pixel to end rectangle  \n",
    "    **Both upper_left and lower_right should be used in (width_pixel, heigth_pixel).** \n",
    "    \n",
    "    * color: Color of rectangle  \n",
    "    * thickness: Thickness of rectangle. If use cv2.FILLED, all the area will be colored.\n",
    "      \n",
    "    \n",
    "* `cv2.circle(img, orgin, redius, color, thickness)`  \n",
    "    * origin: The origin place of the circle  \n",
    "    * redius: Redius of the circle  \n",
    "    * thickness: Thickness of the circle  \n",
    "      \n",
    "    \n",
    "* `cv2.putText(img, text, start_place, font type, scale, color, thickness)`  \n",
    "    * text: Text to be put on  \n",
    "    * start_place: buttom-left corner of the text  \n",
    "    * font type: The type of the text  \n",
    "    * scale: The scale of the text  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "'''\n",
    "# Create a black image\n",
    "0: black\n",
    "255: white\n",
    "'''\n",
    "img_black = np.zeros((512, 512), np.uint8)\n",
    "\n",
    "# Create color image, need 3 channels\n",
    "img_blue = np.zeros((512, 512, 3), np.uint8)\n",
    "img_blue[:] = [255, 0, 0]\n",
    "\n",
    "'''\n",
    "Try out\n",
    "img_blue[200:300, 300, 400] = [255, 0, 0]\n",
    "'''\n",
    "\n",
    "# Add green line\n",
    "img_line = np.zeros((512, 512, 3), np.uint8)\n",
    "cv2.line(img_line, (0, 0), (img.shape[1], img.shape[0]), (0, 255, 0), 3)\n",
    "\n",
    "## Add rectangle\n",
    "img_rec = np.zeros((512, 512, 3), np.uint8)\n",
    "cv2.rectangle(img_rec, (0, 0), (250, 350), (0, 0, 255), 3)\n",
    "cv2.rectangle(img_rec, (250, 350), (img.shape[1], img.shape[0]), (255, 0, 0), cv2.FILLED)\n",
    "\n",
    "## Add Circle\n",
    "img_cir = np.zeros((512, 512, 3), np.uint8)\n",
    "cv2.circle(img_cir, (250, 250), 100, (255, 0, 255), cv2.FILLED)\n",
    "\n",
    "## Add Text\n",
    "img_text = np.zeros((512, 512, 3), np.uint8)\n",
    "cv2.putText(img_text, \"OpenCV\", (300, 100), cv2.FONT_HERSHEY_COMPLEX, 1, (0, 255, 255), 3)\n",
    "\n",
    "# Show the result\n",
    "cv2.imshow(\"Black image\", img_black)\n",
    "cv2.imshow(\"Blue image\", img_blue)\n",
    "cv2.imshow(\"Line\", img_line)\n",
    "cv2.imshow(\"Rectangle\", img_rec)\n",
    "cv2.imshow(\"Circle\", img_cir)\n",
    "cv2.imshow(\"Text\", img_text)\n",
    "\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do what you want to test!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrap perspective  <a name=\"wrap\"></a>\n",
    "\n",
    "[Back to content table](#content)\n",
    "\n",
    "\n",
    "* `cv2.getPerspectiveTransform(pst1, pst2)`  \n",
    "    * pst1: np.float([[old1], [old2], [old3], [old4]])  \n",
    "    * pst2: np.float([[new1], [new2], [new3], [new4]]) \n",
    "     \n",
    "       \n",
    "* `cv2.warpPerspective(img, matrix, (width, height))`  \n",
    "    * img: Old image  \n",
    "    * matrix: Result came from cv2.getPerspective()  \n",
    "    * (width, height): new image size  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "img = cv2.imread(\"resources/pocker.jpg\")\n",
    "\n",
    "cv2.imshow(\"Image\", img)\n",
    "\n",
    "# New size of the output\n",
    "width, height = 250, 350\n",
    "\n",
    "'''\n",
    "pst1: old position\n",
    "pst2: new position\n",
    "transform pst1 -> pst2\n",
    "'''\n",
    "pts1 = np.float32([[239, 103], [356, 146], [179, 277], [296, 320]])\n",
    "pts2 = np.float32([[0, 0], [width, 0], [0, height], [width, height]])\n",
    "\n",
    "pts1 = np.float32([[356, 146], [239, 103], [179, 277], [296, 320]])\n",
    "\n",
    "matrix = cv2.getPerspectiveTransform(pts1, pts2)\n",
    "img_output = cv2.warpPerspective(img, matrix, (width, height))\n",
    "\n",
    "cv2.imshow(\"New image\", img_output)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do what you want to test!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join images <a name=\"join\"/></a>\n",
    "\n",
    "[Back to content table](#content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "img = cv2.imread(\"resources/food.jpg\")\n",
    "\n",
    "# horizotal stack\n",
    "img_hor = np.hstack((img, img))\n",
    "\n",
    "# Vertival stack\n",
    "img_ver = np.vstack((img, img))\n",
    "\n",
    "# Show the result\n",
    "cv2.imshow(\"Horizontal image\", img_hor)\n",
    "cv2.imshow(\"Vertical image\", img_ver)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stackImages(scale, imgArray):\n",
    "    rows = len(imgArray)\n",
    "    cols = len(imgArray[0])\n",
    "\n",
    "    rowsAvailable = isinstance(imgArray[0], list)\n",
    "    \n",
    "    width = imgArray[0][0].shape[1]\n",
    "    height = imgArray[0][0].shape[0]\n",
    "    \n",
    "    if rowsAvailable:\n",
    "        for x in range (0, rows):\n",
    "            for y in range(0, cols):\n",
    "                if imgArray[x][y].shape[:2] == imgArray[0][0].shape[:2]:\n",
    "                    imgArray[x][y] = cv2.resize(imgArray[x][y], (0, 0), None, scale, scale)\n",
    "                else:\n",
    "                    imgArray[x][y] = cv2.resize(imgArray[x][y], (imgArray[0][0].shape[1], imgArray[0][0].shape[0]), None, scale, scale)\n",
    "                    \n",
    "                if len(imgArray[x][y].shape) == 2: \n",
    "                    imgArray[x][y]= cv2.cvtColor(imgArray[x][y], cv2.COLOR_GRAY2BGR)\n",
    "                    \n",
    "        imageBlank = np.zeros((height, width, 3), np.uint8)\n",
    "        \n",
    "        hor = [imageBlank] * rows\n",
    "        \n",
    "        # First put each horizontal image to its horizontal place\n",
    "        for x in range(0, rows):\n",
    "            hor[x] = np.hstack(imgArray[x])\n",
    "        \n",
    "        # Stack each array to the vertical format\n",
    "        ver = np.vstack(hor)\n",
    "    else:\n",
    "        for x in range(0, rows):\n",
    "            if imgArray[x].shape[:2] == imgArray[0].shape[:2]:\n",
    "                imgArray[x] = cv2.resize(imgArray[x], (0, 0), None, scale, scale)\n",
    "            else:\n",
    "                imgArray[x] = cv2.resize(imgArray[x], (imgArray[0].shape[1], imgArray[0].shape[0]), None,scale, scale)\n",
    "                \n",
    "            if len(imgArray[x].shape) == 2: \n",
    "                imgArray[x] = cv2.cvtColor(imgArray[x], cv2.COLOR_GRAY2BGR)\n",
    "                \n",
    "        hor = np.hstack(imgArray)\n",
    "        ver = hor\n",
    "        \n",
    "    return ver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    " \n",
    "img = cv2.imread('resources/food.jpg')\n",
    "img_gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Stack image\n",
    "img_stack = stackImages(0.5, [[img, img_gray, img],[img, img, img]])\n",
    "\n",
    "# Show the image\n",
    "cv2.imshow(\"ImageStack\",img_stack)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do what you want to test!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Color dectection <a name=\"color\"></a>\n",
    "\n",
    "[Back to content table](#content)\n",
    "\n",
    "* `cv2.namedWindow(name, flags)`  \n",
    "    * name: Window's name.  \n",
    "    * flags: Flags of the window. See the documentation for more detail.\n",
    "      \n",
    "        \n",
    "* `cv2.resizeWindow(name, width, height)`  \n",
    "    * name: The name of the window to be resized. \n",
    "    * width: Width after resize.  \n",
    "    * height: Height after resize.  \n",
    "      \n",
    "        \n",
    "* `cv2.createTracker(trackbar_name, window_name, min_value, max_value, callback)`  \n",
    "    * trackbar_name: The name of new trackbar.  \n",
    "    * window_name: The window to add trackbar.  \n",
    "    * min_value: Minimum value of the trackbar. \n",
    "    * max_value: Maximum value of the trackbar.  \n",
    "    * callback: Function to be called when the value of trackbar has changed.  \n",
    "      \n",
    "        \n",
    "* `cv2.getTrackbarPos(trackbar_name, window_name)`\n",
    "    Get the result of the trackbar\n",
    "    * trackbar_name: The target trackbar's name.  \n",
    "    * window_name: The window where the target trackbar exists.  \n",
    "    \n",
    "      \n",
    "* `cv2.inRange(img, lower_value, upper_value)`  \n",
    "    * img: Image to filter.  \n",
    "    * Change the value between lower_value and upper_value to 255 and others to 0.  \n",
    "    \n",
    "      \n",
    "* `cv2.bitwise_and(img1, img2, mask)`\n",
    "    * img1: First input image.  \n",
    "    * img2: Second input image.  \n",
    "    * mask: After doing bitwise_and of img1 and img2 get temp1, do bitwise and with temp1 and get result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Callback function\n",
    "def empty(x):\n",
    "    pass\n",
    "\n",
    "\n",
    "# Create window\n",
    "cv2.namedWindow(\"TrackBars\", cv2.WINDOW_NORMAL)\n",
    "cv2.resizeWindow(\"TrackBars\", 1000, 40)\n",
    "\n",
    "# Create trackbar\n",
    "cv2.createTrackbar(\"Hue Min\", \"TrackBars\", 0, 179, empty)\n",
    "cv2.createTrackbar(\"Hue Max\", \"TrackBars\", 179, 179, empty)\n",
    "cv2.createTrackbar(\"Sat Min\", \"TrackBars\", 0, 255, empty)\n",
    "cv2.createTrackbar(\"Sat Max\", \"TrackBars\", 255, 255, empty)\n",
    "cv2.createTrackbar(\"Value Min\", \"TrackBars\", 0, 255, empty)\n",
    "cv2.createTrackbar(\"Value Max\", \"TrackBars\", 255, 255, empty)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Convert image to HSV format\n",
    "img = cv2.imread(\"resources/car.jpg\")\n",
    "img_hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "# Using the mask to filter the color\n",
    "lower = np.array([0, 93, 0])\n",
    "upper = np.array([10, 255, 255])\n",
    "mask = cv2.inRange(img_hsv, lower, upper)\n",
    "\n",
    "# Result image\n",
    "img_result = cv2.bitwise_and(img, cv2.cvtColor(mask, cv2.COLOR_GRAY2BGR))\n",
    "\n",
    "result = np.hstack([img, cv2.cvtColor(mask, cv2.COLOR_GRAY2BGR), img_result])\n",
    "cv2.imshow(\"Mask image\", result)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)\n",
    "\n",
    "cv2.imwrite(\"result/bitwise.jpg\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def empty(x):\n",
    "    pass\n",
    "\n",
    "\n",
    "# Create window\n",
    "cv2.namedWindow(\"TrackBars\", cv2.WINDOW_NORMAL)\n",
    "cv2.resizeWindow(\"TrackBars\", 1000, 100)\n",
    "\n",
    "# Create trackbar\n",
    "cv2.createTrackbar(\"Hue Min\", \"TrackBars\", 0, 179, empty)\n",
    "cv2.createTrackbar(\"Hue Max\", \"TrackBars\", 179, 179, empty)\n",
    "cv2.createTrackbar(\"Sat Min\", \"TrackBars\", 0, 255, empty)\n",
    "cv2.createTrackbar(\"Sat Max\", \"TrackBars\", 255, 255, empty)\n",
    "cv2.createTrackbar(\"Value Min\", \"TrackBars\", 0, 255, empty)\n",
    "cv2.createTrackbar(\"Value Max\", \"TrackBars\", 255, 255, empty)\n",
    "\n",
    "# Convert image to HSV format\n",
    "img = cv2.imread(\"resources/car.jpg\")\n",
    "img_hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "    \n",
    "while True:\n",
    "    # Get the trackbar value\n",
    "    h_lower = cv2.getTrackbarPos(\"Hue Min\", \"TrackBars\")\n",
    "    h_upper = cv2.getTrackbarPos(\"Hue Max\", \"TrackBars\")\n",
    "    s_lower = cv2.getTrackbarPos(\"Sat Min\", \"TrackBars\")\n",
    "    s_upper = cv2.getTrackbarPos(\"Sat Max\", \"TrackBars\")\n",
    "    v_lower = cv2.getTrackbarPos(\"Value Min\", \"TrackBars\")\n",
    "    v_upper = cv2.getTrackbarPos(\"Value Max\", \"TrackBars\")\n",
    "    \n",
    "    # Print out the value\n",
    "    # print(h_min, h_max, s_min, s_max, v_min, v_max)\n",
    "    \n",
    "    # Using the mask to filter the color\n",
    "    lower = np.array([h_lower, s_lower, v_lower])\n",
    "    upper = np.array([h_upper, s_upper, v_upper])\n",
    "    mask = cv2.inRange(img_hsv, lower, upper)\n",
    "    \n",
    "    # Result image\n",
    "    img_result = cv2.bitwise_and(img, img, mask=mask)\n",
    "    \n",
    "    # Show the image\n",
    "    img_stack = stackImages(0.5, [[img, img_hsv], [mask, img_result]])\n",
    "    cv2.imshow(\"Image\", img_stack)\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'): \n",
    "        break;\n",
    "        \n",
    "\n",
    "print(h_lower, s_lower, v_lower)\n",
    "print(h_upper, s_upper, v_upper)\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do what you want to test!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contour / Shape dectection <a name=\"contour\"></a>\n",
    "\n",
    "[Back to content table](#content)\n",
    "  \n",
    "* `cv2.findContours(img, mode, method)` [Documentation](https://docs.opencv.org/2.4/modules/imgproc/doc/structural_analysis_and_shape_descriptors.html#findcontours)  \n",
    "    * img: Image to find contour.  \n",
    "    * mode: Contour retrieval mode. See the detail in documentation.  \n",
    "    * Method: Contour approximation method. See the detail in documentation.  \n",
    "    \n",
    "  \n",
    "* `cv2.contourArea(contour)` [Documentation](https://docs.opencv.org/2.4/modules/imgproc/doc/structural_analysis_and_shape_descriptors.html#contourarea)  \n",
    "    * contour: Contour get from cv2.findContours(). In fact, it could be 2D array which shape is (n, 2) for storing the vertex information (point vector).\n",
    "    \n",
    "      \n",
    "* `cv2.drawContours(img_output, contour, contourIdx, color, thickness)` [Documentation](https://docs.opencv.org/2.4/modules/imgproc/doc/structural_analysis_and_shape_descriptors.html#drawcontours)  \n",
    "    * img_output: Draw on which image.  \n",
    "    * contour: Point vectors (Same as above).  \n",
    "    * contourIdx: Index for which contour to draw. If it is negative, draw all the contours.  \n",
    "    \n",
    "      \n",
    "* `cv2.arcLength(contour, closed)` [Documentation](https://docs.opencv.org/2.4/modules/imgproc/doc/structural_analysis_and_shape_descriptors.html#arclength)  \n",
    "    * closed: Flag indicating whether the curve is closed or not.  \n",
    "    \n",
    "      \n",
    "* `cv2.approxPolyDP(contour, epsilon, closed)` [Documentation](https://docs.opencv.org/2.4/modules/imgproc/doc/structural_analysis_and_shape_descriptors.html#approxpolydp)  \n",
    "    * epsilon: Specifying the approximation accuracy.  \n",
    "    \n",
    "      \n",
    "* `cv2.boundingRect(points)` [Documentation](https://docs.opencv.org/2.4/modules/imgproc/doc/structural_analysis_and_shape_descriptors.html#boundingrect)  \n",
    "    * points: Point vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_contours(img):\n",
    "    # Retreive the outer contours\n",
    "    contours, hierarchy = cv2.findContours(img, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "    \n",
    "    for cnt in contours:\n",
    "        # Get the area of the closed contour\n",
    "        area = cv2.contourArea(cnt)\n",
    "        \n",
    "        # Clean noise by setting the threshold of area\n",
    "        if area < 800:\n",
    "            continue\n",
    "         \n",
    "        # Draw the contour\n",
    "        cv2.drawContours(img_contour, cnt, -1, (255, 0, 0), 3)\n",
    "        \n",
    "        # Connt the peripheral \n",
    "        peri = cv2.arcLength(cnt, True)\n",
    "        \n",
    "        # Get the vertex of the shape\n",
    "        approx = cv2.approxPolyDP(cnt, 0.02 * peri, True)\n",
    "        \n",
    "        # The number of the sides\n",
    "        obj_cor = len(approx)\n",
    "        \n",
    "        # Get the bounding box of the shape\n",
    "        x, y, w, h = cv2.boundingRect(approx)\n",
    "        \n",
    "        # Detect the shape\n",
    "        if obj_cor == 3:\n",
    "            object_type = \"Tri\"\n",
    "        elif obj_cor == 4:\n",
    "            # Check whether is circle or rectangle\n",
    "            asp_ratio = w / h\n",
    "            \n",
    "            if (asp_ratio > 0.95) & (asp_ratio < 1.05):\n",
    "                object_type = \"Square\"\n",
    "            else:\n",
    "                object_type = \"Rectangle\"\n",
    "        elif obj_cor == 5:\n",
    "            object_type = \"Pentagon\"\n",
    "        elif obj_cor == 6:\n",
    "            object_type = \"Hexagon\"\n",
    "        else:\n",
    "            object_type = \"Circle\"\n",
    "        \n",
    "        # Drow the bounding box and put the shape text on it\n",
    "        cv2.rectangle(img_contour, (x, y), (x + w, y + h), (0, 0, 255), 3)\n",
    "        cv2.putText(img_contour, object_type,\n",
    "                    (x + (w // 2) - 20, y - 15), cv2.FONT_HERSHEY_COMPLEX, 0.5,\n",
    "                    (0, 0, 0), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "img = cv2.imread(\"resources/shapes.jpg\")\n",
    "\n",
    "# Change the image to gray scale and blur the image\n",
    "img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "img_blur = cv2.GaussianBlur(img_gray, (3, 3), 1)\n",
    "\n",
    "# Edge detection\n",
    "img_canny = cv2.Canny(img_blur, 50, 50)\n",
    "\n",
    "# Copy the original image\n",
    "img_contour = img.copy()\n",
    "\n",
    "# Get contours\n",
    "get_contours(img_canny)\n",
    "\n",
    "# Show the image\n",
    "img_stack = stackImages(0.5, [[img, img_blur], [img_canny, img_contour]])\n",
    "cv2.imshow(\"Result\", img_stack)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)\n",
    "\n",
    "cv2.imwrite(\"contours.jpg\", img_stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do what you want to test!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Object detection <a name=\"object\"></a>\n",
    "\n",
    "[Back to content table](#content)\n",
    "\n",
    "* `cv2.CascadeClassifier(file)` [Documentation](https://docs.opencv.org/2.4/modules/objdetect/doc/cascade_classification.html#cascadeclassifier-cascadeclassifier)   \n",
    "    Loads a classifier from a file.\n",
    "    * file: Place to load the xml file.  \n",
    "    \n",
    "      \n",
    "* `cv2.CascadeClassifier.detectMultiScale(img, scaleFactor, minNeighbors)` [Documentation](https://docs.opencv.org/2.4/modules/objdetect/doc/cascade_classification.html#cascadeclassifier-detectmultiscale)  \n",
    "    * img: Image to detect.\n",
    "    * scaleFactor: Specifying how much the image size is reduced at each image scale.  \n",
    "    * minNeighbors: Specifying how many neighbors each candidate rectangle should have to retain it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Read in the xml file\n",
    "face_cascade = cv2.CascadeClassifier(\"cascade_files/haarcascade_frontalface_default.xml\")\n",
    "\n",
    "img = cv2.imread(\"resources/me.jpg\")\n",
    "\n",
    "# Convert into gray scale\n",
    "img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Dectect the face\n",
    "faces = face_cascade.detectMultiScale(img_gray, 1.1, 5)\n",
    "\n",
    "# Draw the bounding boxes\n",
    "for (x, y, w, h) in faces:\n",
    "    cv2.rectangle(img, (x, y), (x + w, y + h), (255, 0, 0), 2)\n",
    "    \n",
    "cv2.imshow(\"Result\", img)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do what you want to test!"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
