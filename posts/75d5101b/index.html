<!DOCTYPE html>
<html lang="en">
    <head>
    <meta charset="utf-8">

    

    <!-- 渲染优化 -->
    <meta name="renderer" content="webkit">
    <meta name="force-rendering" content="webkit">
    <meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1">
    <meta name="HandheldFriendly" content="True" >
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

    <!--icon-->

    
    
    
        <link rel="icon" type="image/png" sizes="32x32" href="/icon.png">
    
    
        <link rel="apple-touch-icon" sizes="180x180" href="/apple_icon_128.png">
    
    
        <link rel="mask-icon" href="/apple_icon_128.png">
    


    <!-- meta -->


<title>[Series - 12] Regression - 1 | Justin&#39;s sharing place</title>


    <meta name="keywords" content="OpenCV,MachineLearning, Android, AI, Algorithm, DataStructure">




    <!-- OpenGraph -->
 
    <meta name="description" content="[Series - 12] Regression - 1See moreBack to the series page   前言接下來三篇我都會分享有關 Linear Regression 的知識，這篇為基本的介紹，後兩篇為數學方面與實作的分享。 NotebookNotebook Regression假設我們現在有一些不連續的資料點，當我們把資料點透過圖的方式呈現出來後，可以看到很多不連續的點散落">
<meta property="og:type" content="article">
<meta property="og:title" content="[Series - 12] Regression - 1">
<meta property="og:url" content="justin900429.github.io/posts/75d5101b/index.html">
<meta property="og:site_name" content="Justin&#39;s sharing place">
<meta property="og:description" content="[Series - 12] Regression - 1See moreBack to the series page   前言接下來三篇我都會分享有關 Linear Regression 的知識，這篇為基本的介紹，後兩篇為數學方面與實作的分享。 NotebookNotebook Regression假設我們現在有一些不連續的資料點，當我們把資料點透過圖的方式呈現出來後，可以看到很多不連續的點散落">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://ithelp.ithome.com.tw/upload/images/20200829/20129593e788c1Xq0r.jpg">
<meta property="og:image" content="https://ithelp.ithome.com.tw/upload/images/20200830/20129593edF8iqa0Ei.jpg">
<meta property="og:image" content="https://ithelp.ithome.com.tw/upload/images/20200830/20129593wNNVpFD7tn.jpg">
<meta property="og:image" content="https://ithelp.ithome.com.tw/upload/images/20200830/20129593jkOeW5Z4U2.jpg">
<meta property="og:image" content="https://ithelp.ithome.com.tw/upload/images/20200830/201295937FlndIhhRC.jpg">
<meta property="og:image" content="https://ithelp.ithome.com.tw/upload/images/20200830/20129593mmDcvt4Vs0.jpg">
<meta property="article:published_time" content="2020-08-30T09:48:19.000Z">
<meta property="article:modified_time" content="2021-01-01T02:29:52.021Z">
<meta property="article:author" content="Justin Ruan">
<meta property="article:tag" content="OpenCV,MachineLearning, Android, AI, Algorithm, DataStructure">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://ithelp.ithome.com.tw/upload/images/20200829/20129593e788c1Xq0r.jpg">


    
<link rel="stylesheet" href="/css/style/main.css">
 



    
    
    
        <link rel="stylesheet" id="hl-default-theme" href="https://cdn.jsdelivr.net/npm/highlight.js@10.1.2/styles/atom-one-light.min.css" media="none" onload="if(getComputedStyle(document.documentElement).getPropertyValue('--color-mode').indexOf('dark')===-1)this.media='all'">
        
    

    

     

    <!-- custom head -->

<meta name="generator" content="Hexo 5.1.1"></head>

    <body>
        <div id="app">
            <script>
window.onscroll = () => {
    const nav = document.querySelector('.header');
    if(this.scrollY <= 10) {
        nav.style.backgroundColor = "rgba(255, 255, 255, 0)"
    } else {
        nav.style.backgroundColor = "rgba(255, 255, 255, 0.9)"
    }
};
</script>

<header class="header">
    <div class="header__left">
        <a href="/" class="button">
            <span class="logo__text">Justin&#39;s blog</span>
        </a>
    </div>
    <div class="header__right">
        
            <div class="navbar__menus">
                
                    <a href="/about/" class="navbar-menu button">Home</a>
                
                    <a href="/about/resume.pdf" class="navbar-menu button">CV</a>
                
                    <a href="/" class="navbar-menu button">Articles</a>
                
                    <a href="/tags/" class="navbar-menu button">Tags</a>
                
                    <a href="/archives/" class="navbar-menu button">Archives</a>
                
            </div>
        
        
        
    <a href="/search/" id="btn-search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 1024 1024" width="24" height="24" fill="currentColor" stroke="currentColor" stroke-width="32"><path d="M192 448c0-141.152 114.848-256 256-256s256 114.848 256 256-114.848 256-256 256-256-114.848-256-256z m710.624 409.376l-206.88-206.88A318.784 318.784 0 0 0 768 448c0-176.736-143.264-320-320-320S128 271.264 128 448s143.264 320 320 320a318.784 318.784 0 0 0 202.496-72.256l206.88 206.88 45.248-45.248z"></path></svg>
    </a>


        
        

         
    <a href="#" class="button" id="b2t" aria-label="Back to Top" title="Back to Top">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 1024 1024" width="32" height="32">
            <path d="M233.376 722.752L278.624 768 512 534.624 745.376 768l45.248-45.248L512 444.128zM192 352h640V288H192z" fill="currentColor"></path>
        </svg>
    </a>


        
            <a class="dropdown-icon button" id="btn-dropdown" tabindex="0"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" width='24' height='24' fill="none" stroke="currentColor" stroke-width="0.7" stroke-linecap="round" stroke-linejoin="round"><path fill="currentColor" d="M3.314,4.8h13.372c0.41,0,0.743-0.333,0.743-0.743c0-0.41-0.333-0.743-0.743-0.743H3.314c-0.41,0-0.743,0.333-0.743,0.743C2.571,4.467,2.904,4.8,3.314,4.8z M16.686,15.2H3.314c-0.41,0-0.743,0.333-0.743,0.743s0.333,0.743,0.743,0.743h13.372c0.41,0,0.743-0.333,0.743-0.743S17.096,15.2,16.686,15.2z M16.686,9.257H3.314c-0.41,0-0.743,0.333-0.743,0.743s0.333,0.743,0.743,0.743h13.372c0.41,0,0.743-0.333,0.743-0.743S17.096,9.257,16.686,9.257z"></path></svg></a>
            <div class="dropdown-menus" id="dropdown-menus">
                
                    <a href="/about/" class="dropdown-menu button">Home</a>
                
                    <a href="/about/resume.pdf" class="dropdown-menu button">CV</a>
                
                    <a href="/" class="dropdown-menu button">Articles</a>
                
                    <a href="/tags/" class="dropdown-menu button">Tags</a>
                
                    <a href="/archives/" class="dropdown-menu button">Archives</a>
                
            </div>
        
    </div>
</header>


            <main class="main">
    

<div class="post-title">
    <h1 class="post-title__text">
        [Series - 12] Regression - 1
    </h1>
    <div class="post-title__meta">
        <a href="/archives/2020/08/" class="post-meta__date button">2020-08-30</a>
        
 
        
    
    


 

 
    </div>
</div>


    <aside class="post-side">
        <div class="post-side__toc">
            <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%89%8D%E8%A8%80"><span class="toc-number">1.</span> <span class="toc-text">前言</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Notebook"><span class="toc-number">2.</span> <span class="toc-text">Notebook</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Regression"><span class="toc-number">3.</span> <span class="toc-text">Regression</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Linear-Regression"><span class="toc-number">4.</span> <span class="toc-text">Linear Regression</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Linear-function"><span class="toc-number">4.1.</span> <span class="toc-text">Linear function</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Loss-function"><span class="toc-number">4.2.</span> <span class="toc-text">Loss function</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Scikit-learn"><span class="toc-number">4.3.</span> <span class="toc-text">Scikit-learn</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Multnomial-regression"><span class="toc-number">5.</span> <span class="toc-text">Multnomial regression</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Polynomial-Regression"><span class="toc-number">6.</span> <span class="toc-text">Polynomial Regression</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%B0%8F%E7%B5%90"><span class="toc-number">7.</span> <span class="toc-text">小結</span></a></li></ol>
        </div>
    </aside>
    <a class="btn-toc button" id="btn-toc" tabindex="0">
        <svg viewBox="0 0 1024 1024" width="32" height="32" xmlns="http://www.w3.org/2000/svg">
            <path d="M128 256h64V192H128zM320 256h576V192H320zM128 544h64v-64H128zM320 544h576v-64H320zM128 832h64v-64H128zM320 832h576v-64H320z" fill="currentColor"></path>
        </svg>
    </a>
    <div class="toc-menus" id="toc-menus">
        <div class="toc-title">Article Directory</div>
        <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%89%8D%E8%A8%80"><span class="toc-number">1.</span> <span class="toc-text">前言</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Notebook"><span class="toc-number">2.</span> <span class="toc-text">Notebook</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Regression"><span class="toc-number">3.</span> <span class="toc-text">Regression</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Linear-Regression"><span class="toc-number">4.</span> <span class="toc-text">Linear Regression</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Linear-function"><span class="toc-number">4.1.</span> <span class="toc-text">Linear function</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Loss-function"><span class="toc-number">4.2.</span> <span class="toc-text">Loss function</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Scikit-learn"><span class="toc-number">4.3.</span> <span class="toc-text">Scikit-learn</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Multnomial-regression"><span class="toc-number">5.</span> <span class="toc-text">Multnomial regression</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Polynomial-Regression"><span class="toc-number">6.</span> <span class="toc-text">Polynomial Regression</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%B0%8F%E7%B5%90"><span class="toc-number">7.</span> <span class="toc-text">小結</span></a></li></ol>
    </div>


<article class="post content-card">
    <div class="post__header"></div>
    <div class="post__content">
        <h1 id="Series-12-Regression-1"><a href="#Series-12-Regression-1" class="headerlink" title="[Series - 12] Regression - 1"></a>[Series - 12] Regression - 1</h1><blockquote class="blockquote-note blockquote-note__tip"><div class="blockquote-note__header"><div class="blockquote-note__icon"><svg xmlns="http://www.w3.org/2000/svg" width="12" height="16" viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></div>See more</div><div class="blockquote-note__content"><p><a href="/posts/f9a1d882/">Back to the series page</a></p>
</div></blockquote>

<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>接下來三篇我都會分享有關 Linear Regression 的知識，這篇為基本的介紹，後兩篇為數學方面與實作的分享。</p>
<h2 id="Notebook"><a href="#Notebook" class="headerlink" title="Notebook"></a>Notebook</h2><p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://colab.research.google.com/github/Justin900429/IT-used/blob/master/Regression.ipynb">Notebook</a></p>
<h2 id="Regression"><a href="#Regression" class="headerlink" title="Regression"></a>Regression</h2><p>假設我們現在有一些<strong>不連續的資料點</strong>，當我們把資料點透過<strong>圖</strong>的方式呈現出來後，可以看到很多不連續的點散落在圖形上。我們同時也知道<strong>函數</strong>也是可以透過<strong>圖形</strong>呈現出來。那我們就會猜測，我們是否能找到一個函數，可以就某種程度上預測未來或未知資料的值，這種方式就叫 <strong>Regression</strong>。</p>
<blockquote>
<p>之後會講的 Logistic Regression 雖然是用於分類，但是他其實跟 Regression 也有很大的關係！</p>
</blockquote>
<h2 id="Linear-Regression"><a href="#Linear-Regression" class="headerlink" title="Linear Regression"></a>Linear Regression</h2><h3 id="Linear-function"><a href="#Linear-function" class="headerlink" title="Linear function"></a>Linear function</h3><p><strong>Linear</strong>是指利用直線來預測資料，首先我們先來產生一些資料點：</p>
<pre><code class="hljs python"><span class="hljs-comment"># Linear space between 0 to 10</span>
x = np.linspace(<span class="hljs-number">6</span>, <span class="hljs-number">10</span>, <span class="hljs-number">20</span>)
<span class="hljs-comment"># Generate data from 2 * x + 10 with some gaussian noise</span>
y = <span class="hljs-number">2</span> * x + <span class="hljs-number">10</span> + np.random.normal(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, x.size())

<span class="hljs-comment"># generate scatter plot</span>
plt.scatter(x, y)</code></pre>

<p><img src="https://ithelp.ithome.com.tw/upload/images/20200829/20129593e788c1Xq0r.jpg" alt="https://ithelp.ithome.com.tw/upload/images/20200829/20129593e788c1Xq0r.jpg"></p>
<blockquote>
<p>這裡我們的 noise 用 Gaussian 是有原因的，不過我們就留到明天說吧！</p>
</blockquote>
<p>假設我們不知道我們這些點是怎麼產生的，我們現在試著用一條<strong>線性函數</strong>來預測這些點，使得說這些點是由這條線所產生的<strong>機率</strong>最大。</p>
<h3 id="Loss-function"><a href="#Loss-function" class="headerlink" title="Loss function"></a>Loss function</h3><p>座標上的直線有無限多條，我們必須要有一個衡量的方式，來說明我們的直線是最好的、最能代表的，這就是 <strong>Loss function</strong> 的函數意義。<strong>Loss</strong> 就是損失的意思，指的就是我們預測跟原本資料差多少。一般我們在 Regression 有<strong>L1 Loss</strong> 跟 <strong>L2 Loss</strong>，數學形式分別為：</p>
<p>$$<br>\text{L2 Loss} :\ L(m, b) = \frac{1}{n}\sum_{k=1}^{n}(\hat{y} - y_{k})^{2}<br>$$<br>$$<br>\text{L1 Loss} :\ L(m, b) = \frac{1}{n}\sum_{k=1}^n\mid\mid\hat{y} - y_{k}\mid\mid<br>$$</p>
<p>挑選這兩個函數是因為這兩的函數都具有<strong>谷點</strong>，因此可以找到最小值！<br><img src="https://ithelp.ithome.com.tw/upload/images/20200830/20129593edF8iqa0Ei.jpg" alt="https://ithelp.ithome.com.tw/upload/images/20200830/20129593edF8iqa0Ei.jpg"></p>
<p>接著我們的目標就是<strong>找到一條直線使得Loss function最小</strong>！因此我們可以利用偏微分的方式或是利用 <strong>Normal Equation</strong> 來求的最佳解，這些部分就留到明天說把！</p>
<h3 id="Scikit-learn"><a href="#Scikit-learn" class="headerlink" title="Scikit-learn"></a>Scikit-learn</h3><p>現在我們直接用 <strong>Scikit-learn</strong> 來幫我們處理這些問題吧！</p>
<pre><code class="hljs python"><span class="hljs-comment"># import scikit learn package</span>
<span class="hljs-keyword">from</span> sklearn.linear_model <span class="hljs-keyword">import</span> LinearRegression

<span class="hljs-comment"># generate data</span>
x = np.linspace(<span class="hljs-number">6</span>, <span class="hljs-number">10</span>, <span class="hljs-number">20</span>)
y = <span class="hljs-number">2</span> * x + <span class="hljs-number">10</span> + np.random.normal(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, x.size)

<span class="hljs-comment"># Create an linear regression omptimizer </span>
<span class="hljs-comment"># Data of x should be in 2d, so we should add new axis to x</span>
reg = LinearRegression().fit(x[:, np.newaxis], y)

print(<span class="hljs-string">f&quot;The regression line is y = <span class="hljs-subst">&#123;reg.coef_[<span class="hljs-number">0</span>]&#125;</span>x + <span class="hljs-subst">&#123;reg.intercept_&#125;</span>&quot;</span>)

plt.figure(figsize=(<span class="hljs-number">10</span>, <span class="hljs-number">8</span>))

<span class="hljs-comment"># Plot the scatter plot for origin data</span>
plt.scatter(x, y, c=<span class="hljs-string">&quot;red&quot;</span>, label=<span class="hljs-string">&quot;origin data&quot;</span>)
<span class="hljs-comment"># Plot the prediction line</span>
plt.plot(x, reg.coef_[<span class="hljs-number">0</span>] * x + reg.intercept_, label=<span class="hljs-string">&quot;prediction line&quot;</span>)

<span class="hljs-comment"># For showing the labels</span>
plt.legend(prop=&#123;<span class="hljs-string">&#x27;size&#x27;</span>: <span class="hljs-number">15</span>&#125;)</code></pre>

<p>基本上 <strong>Scikit-learn</strong> 要去調適一個模型時的步驟都是如下:</p>
<pre><code class="hljs python">model = &lt;Algo you use&gt;()
model.fit(&lt;data-of-x&gt;, &lt;label&gt;)

<span class="hljs-comment"># For instance</span>
model = LinearRegression()
model.fit(x[:, np.newaxis], y)</code></pre>

<p>另外，要注意的是我們的<strong>x必須是2d</strong>，我們等等會來說為什麼！<br><img src="https://ithelp.ithome.com.tw/upload/images/20200830/20129593wNNVpFD7tn.jpg" alt="https://ithelp.ithome.com.tw/upload/images/20200830/20129593wNNVpFD7tn.jpg"></p>
<h2 id="Multnomial-regression"><a href="#Multnomial-regression" class="headerlink" title="Multnomial regression"></a>Multnomial regression</h2><p>現在的問題變成說，我們的資料種類不只兩種，而是有兩種以上。其實我們剛剛上面的操作只在<strong>2維</strong>，當到<strong>3維</strong>時，我們就可以用平面來處理！</p>
<p>這邊我是用 Sciki-learn 裡的 boston dataset，並且只取兩個特徵。同樣利用 <strong>Linear Regression</strong> 來做預測。</p>
<pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn <span class="hljs-keyword">import</span> datasets
<span class="hljs-keyword">from</span> sklearn.linear_model <span class="hljs-keyword">import</span> LinearRegression
<span class="hljs-keyword">from</span> sklearn.preprocessing <span class="hljs-keyword">import</span> StandardScaler
<span class="hljs-keyword">from</span> mpl_toolkits <span class="hljs-keyword">import</span> mplot3d

<span class="hljs-comment"># Get boston dataset and leave only 2 features</span>
data = datasets.load_boston()
x = pd.DataFrame(data.data, columns=data.feature_names)
x = x.loc[:, [<span class="hljs-string">&quot;DIS&quot;</span>, <span class="hljs-string">&quot;NOX&quot;</span>,]]

<span class="hljs-comment"># Normalize the data</span>
x = StandardScaler().fit_transform(x)[:<span class="hljs-number">100</span>]

<span class="hljs-comment"># Fit the data to Linear Regression model</span>
model = LinearRegression().fit(x, data.target[:<span class="hljs-number">100</span>])

<span class="hljs-comment"># 3d plot</span>
fig = plt.figure()
ax = plt.axes(projection=<span class="hljs-string">&#x27;3d&#x27;</span>)
ax.scatter3D(x[:, <span class="hljs-number">0</span>], x[:, <span class="hljs-number">1</span>], data.target[:<span class="hljs-number">100</span>])
xx, yy = np.meshgrid(np.linspace(<span class="hljs-number">0</span>, <span class="hljs-number">2.5</span>, <span class="hljs-number">10</span>), np.linspace(<span class="hljs-number">-0.2</span>, <span class="hljs-number">-1.5</span>, <span class="hljs-number">10</span>))
ax.plot_surface(xx, yy,
                xx * model.coef_[<span class="hljs-number">0</span>] + yy * model.coef_[<span class="hljs-number">1</span>] + model.intercept_, 
                color=<span class="hljs-string">&quot;red&quot;</span>,
                alpha=<span class="hljs-number">0.5</span>)</code></pre>

<p><img src="https://ithelp.ithome.com.tw/upload/images/20200830/20129593jkOeW5Z4U2.jpg" alt="https://ithelp.ithome.com.tw/upload/images/20200830/20129593jkOeW5Z4U2.jpg"></p>
<p>這邊只選用2個特徵是為了要畫圖讓大家看說高維的Linear Regression，如果把所有資料點丟進去也是可以做預測的！事實上，現在的數學形式會變這樣：</p>
<p>$$<br>y^{(i)} = \theta_{1}x_{1}^{(i)} + \theta_{2}x_{2}^{(i)} + … + \theta_{n}x_{n}^{(i)}<br>$$<br>$$<br>Note: x_{m}^{(i)} \ \text{代表第 i 筆的第 m 個特徵}<br>$$</p>
<p>而其 Loss function 則同樣不變，為利用 <strong>L2 Loss</strong>。</p>
<h2 id="Polynomial-Regression"><a href="#Polynomial-Regression" class="headerlink" title="Polynomial Regression"></a>Polynomial Regression</h2><p>這次我們的函數不再侷限於 <strong>Linear</strong> ，而是可以拓展到 <strong>Polynomial</strong>。這代表我們的預測可以隨著愈高愈準確（但可能會 Overfit)。 同樣來生成資料點吧！</p>
<pre><code class="hljs python">X = np.linspace(<span class="hljs-number">0</span>, <span class="hljs-number">5</span>, <span class="hljs-number">20</span>)
y = (X ** <span class="hljs-number">3</span>) - <span class="hljs-number">5</span> * (X ** <span class="hljs-number">2</span>)  - <span class="hljs-number">3</span> * X - <span class="hljs-number">5</span>

plt.scatter(X, y + <span class="hljs-number">2</span> * np.random.randn(X.size), c=<span class="hljs-string">&quot;red&quot;</span>)</code></pre>

<p><img src="https://ithelp.ithome.com.tw/upload/images/20200830/201295937FlndIhhRC.jpg" alt="https://ithelp.ithome.com.tw/upload/images/20200830/201295937FlndIhhRC.jpg"></p>
<p>我們知道多項式的數學形式是長這樣:</p>
<p>$$<br>y = a_{n}x^{n} + a_{n - 1}x^{n - 1} + … + a_{1}x + a_{0}<br>$$</p>
<p>這時，我們就可以試驗不同次方來擬合這些資料點，並分析哪一個會比較好。另外，<strong>Polynomial Regression</strong> 跟 <strong>Multinomial Regression</strong> 其實本質上來說是一樣的，只要做個對應：</p>
<p>$$<br>x^{n} \rightarrow x_{n},<br> \ x^{n - 1} \rightarrow x_{n - 1}, \<br>…<br>$$</p>
<p>就變成 Multinomial 了！因此順序是</p>
<pre><code class="hljs plain">poly -&gt; multi -&gt;(using linear regression to predict)</code></pre>

<p>而 <strong>Sklearn</strong> 已經有寫好的函數可以幫我們算出多項式的各種組合！看下方的 <code>PolynomialFeatures</code>，建議大家可以把下方的 <code>print(x)</code>解開，看看經過這個變換後會出現什麼！</p>
<pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.preprocessing <span class="hljs-keyword">import</span> PolynomialFeatures
<span class="hljs-keyword">from</span> sklearn.linear_model <span class="hljs-keyword">import</span> LinearRegression

<span class="hljs-comment"># Generate data points</span>
X = np.linspace(<span class="hljs-number">0</span>, <span class="hljs-number">3</span>, <span class="hljs-number">20</span>)
y = (X ** <span class="hljs-number">3</span>) - <span class="hljs-number">3</span> * (X ** <span class="hljs-number">2</span>) + <span class="hljs-number">3</span> * X - <span class="hljs-number">5</span> + np.random.randn(X.size)

<span class="hljs-comment"># Set different degree</span>
degrees = [<span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">15</span>]

plt.figure(figsize=(<span class="hljs-number">20</span>, <span class="hljs-number">10</span>))

count = <span class="hljs-number">1</span>
<span class="hljs-keyword">for</span> degree <span class="hljs-keyword">in</span> degrees:
    <span class="hljs-comment"># Transfrom x to [1, x, x^2, x^3, ...]</span>
    x = PolynomialFeatures(degree).fit_transform(X[:, np.newaxis])
    
    <span class="hljs-comment"># Using linear gression to predict the model</span>
    model = LinearRegression().fit(x, y)
    
    plt.subplot(<span class="hljs-number">1</span>, <span class="hljs-number">3</span>, count)
    plt.title(<span class="hljs-string">&quot;Degree&#123;&#125;&quot;</span>.format(degree))
    <span class="hljs-comment"># Origin points</span>
    plt.scatter(X, y)
    <span class="hljs-comment"># Predict points</span>
    plt.plot(X, model.predict(x))
    
    <span class="hljs-comment"># Next plot</span>
    count += <span class="hljs-number">1</span></code></pre>

<p><img src="https://ithelp.ithome.com.tw/upload/images/20200830/20129593mmDcvt4Vs0.jpg" alt="https://ithelp.ithome.com.tw/upload/images/20200830/20129593mmDcvt4Vs0.jpg"></p>
<p>上面三張圖最左邊的擬和程度最好，但是這有可能是 <strong>model 過度預測了</strong>！我們後面會說明什麼是 <strong>Overfitting</strong>，但是可以想像說，今天我們有新得資料點要來預測，最左邊的預測能力可能會下降，因為他沒有所謂的<strong>普遍性</strong>，只能適應訓練的資料！</p>
<h2 id="小結"><a href="#小結" class="headerlink" title="小結"></a>小結</h2><p>今天跟大家討論完關於 Regression 的一些應用，明天則會說明數學的證明以及想法。如果有任何疑問或是想跟大家討論的地方，歡迎下方留言。如果有任何錯誤，也請跟我們說，那下篇見！</p>
<br/>
<br/>
<a href="/posts/df7aa4ac/" float=left>Last</a> <a href="/posts/6b7a8ae2/" style="float: right;">Next</a>
<!-- flag of hidden posts -->
    </div>
     
    <div class="post-footer__meta"><p>updated at 2021-01-01</p></div> 
    <div class="post-meta__cats"></div> 
</article>




    <div class="post__comments content-card" id="comment">
        
    <h4>Comments</h4>
    
    <div id="disqus_thread">Unable to load Disqus, please make sure your network can access.</div>

    
    
    
    
    
    
    
    
    


    </div>



</main>

            <footer class="footer">
    
        <script src='https://cdnjs.cloudflare.com/ajax/libs/viz.js/1.7.1/viz.js'></script>
        <script>
          String.prototype.replaceAll = function(search, replacement) {
            var target = this;
            return target.split(search).join(replacement);
          };
      
          let vizObjects = document.querySelectorAll('.graphviz')
      
          for (let item of vizObjects) {
            let svg = undefined
            try {
              svg = Viz(item.textContent.replaceAll('–', '--'), 'svg')
            } catch(e) {
              svg = `<pre class="error">${e}</pre>`
            }
            item.outerHTML = svg
          }
        </script>
    
    


    
     
 

 
    
        
        <p class="footer-copyright">
            Copyright © 2020&nbsp;-&nbsp;2021 <a href="/">Justin&#39;s sharing place</a>
        </p>
    
    
    <p>Powered by <a href="https://hexo.io" rel="external nofollow noreferrer" target="_blank">Hexo</a> | Theme - <a href="https://github.com/ChrAlpha/hexo-theme-cards" rel="external nofollow noreferrer" target="_blank">Cards</a></p>
</footer>

        </div>
         

 
    <script>
        window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
        ga('create', 'G-4FZNQ241GW', 'auto');
        ga('send', 'pageview');
    </script>
    <script async src="https://www.google-analytics.com/analytics.js"></script>
 

 

 

  



 


    
 

 


    <script>
        if (typeof MathJax === 'undefined') {
            window.MathJax = {
                loader: {
                    source: {
                        '[tex]/amsCd': '[tex]/amscd',
                        '[tex]/AMScd': '[tex]/amscd'
                    }
                },
                tex: {
                    inlineMath: [['$', '$'], ['\\(', '\\)']],
                },
                options: {
                    renderActions: {
                        findScript: [10, doc => {
                            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
                                const display = !!node.type.match(/; *mode=display/);
                                const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
                                const text = document.createTextNode('');
                                node.parentNode.replaceChild(text, node);
                                math.start = {node: text, delim: '', n: 0};
                                math.end = {node: text, delim: '', n: 0};
                                doc.math.push(math);
                            });
                        }, '', false],
                        insertedScript: [200, () => {
                            document.querySelectorAll('mjx-container').forEach(node => {
                                let target = node.parentNode;
                                if (target.nodeName.toLowerCase() === 'li') {
                                    target.parentNode.classList.add('has-jax');
                                }
                            });
                        }, '', false]
                    }
                }
            };
            (function () {
                var script = document.createElement('script');
                script.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
                script.defer = true;
                document.head.appendChild(script);
            })();
        } else {
            MathJax.startup.document.state(0);
            MathJax.texReset();
            MathJax.typeset();
        }
    </script>
 

 

 


    
    <script>
        function loadComment() {
            window.disqus_config = function () {
                this.page.url = 'justin900429.github.io/posts/75d5101b/';
                this.page.identifier = 'posts/75d5101b/';
            };
            (function(){
                var dsq = document.createElement('script');
                dsq.type = 'text/javascript';
                dsq.async = true;
                dsq.src = '//' + 'https-justin900429-github-io' + '.disqus.com/embed.js';
                (document.head || document.body).appendChild(dsq);
            })();
        }
    
        var runningOnBrowser = typeof window !== "undefined";
        var isBot = runningOnBrowser && !("onscroll" in window) || typeof navigator !== "undefined" && /(gle|ing|ro|msn)bot|crawl|spider|yand|duckgo/i.test(navigator.userAgent);
        var supportsIntersectionObserver = runningOnBrowser && "IntersectionObserver" in window;
    
        setTimeout(function () {
            if (!isBot && supportsIntersectionObserver) {
                var comment_observer = new IntersectionObserver(function(entries) {
                    if (entries[0].isIntersecting) {
                        loadComment();
                        comment_observer.disconnect();
                    }
                }, { threshold: [0] });
                comment_observer.observe(document.getElementById('comment'));
            } else {
                loadComment();
            }
        }, 1);
    </script>


    
    
    

    
    
    
    
    

    
    
    
    
    

    



    </body>
</html>
