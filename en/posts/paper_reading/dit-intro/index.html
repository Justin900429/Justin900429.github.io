<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Diffusion Transformer: A powerful archiecture for image generation | Justin's Site</title><meta name=keywords content="Diffusion,Image Generation,Transformer,DiT,Diffusion Transformer,pixart-alpha,Generative Models,Code"><meta name=description content="An introduction to the Diffusion Transformer (DiT) architecture with examples of its applications in image generation."><meta name=author content="Justin"><link rel=canonical href=https://justin900429.github.io/en/posts/paper_reading/dit-intro/><link crossorigin=anonymous href=/assets/css/stylesheet.0fd896da64c856c1d033709781aee14ceeedda323fa9830eaf98bbabf131b313.css integrity="sha256-D9iW2mTIVsHQM3CXga7hTO7t2jI/qYMOr5i7q/ExsxM=" rel="preload stylesheet" as=style><link rel=icon href=https://justin900429.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://justin900429.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://justin900429.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://justin900429.github.io/apple-touch-icon.png><link rel=mask-icon href=https://justin900429.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css integrity=sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js integrity=sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4 crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js integrity=sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa crossorigin=anonymous onload=renderMathInElement(document.body)></script><meta property="og:title" content="Diffusion Transformer: A powerful archiecture for image generation"><meta property="og:description" content="An introduction to the Diffusion Transformer (DiT) architecture with examples of its applications in image generation."><meta property="og:type" content="article"><meta property="og:url" content="https://justin900429.github.io/en/posts/paper_reading/dit-intro/"><meta property="og:image" content="https://justin900429.github.io/images/generate.png"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-09-18T18:45:59+08:00"><meta property="article:modified_time" content="2024-09-18T18:45:59+08:00"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://justin900429.github.io/images/generate.png"><meta name=twitter:title content="Diffusion Transformer: A powerful archiecture for image generation"><meta name=twitter:description content="An introduction to the Diffusion Transformer (DiT) architecture with examples of its applications in image generation."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"üìö article","item":"https://justin900429.github.io/en/posts/"},{"@type":"ListItem","position":2,"name":"üìñ paper-reading","item":"https://justin900429.github.io/en/posts/paper_reading/"},{"@type":"ListItem","position":3,"name":"Diffusion Transformer: A powerful archiecture for image generation","item":"https://justin900429.github.io/en/posts/paper_reading/dit-intro/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Diffusion Transformer: A powerful archiecture for image generation","name":"Diffusion Transformer: A powerful archiecture for image generation","description":"An introduction to the Diffusion Transformer (DiT) architecture with examples of its applications in image generation.","keywords":["Diffusion","Image Generation","Transformer","DiT","Diffusion Transformer","pixart-alpha","Generative Models","Code"],"articleBody":"Introduction In recent developments, an increasing number of open-source models for image generation have adopted the Diffusion Transformer (DiT) ( Citation: Peebles \u0026 Xie, 2023 Peebles, W. \u0026 Xie, S. (2023). Scalable diffusion models with transformers. IEEE/CVF International Conference on Computer Vision (ICCV). ) as their backbone. Notable examples include Stable Diffusion 3 (SD3), Sora, Open-Sora, and FLUX. These models demonstrate significant improvements in image quality over previous architectures. DiT represents an advancement by replacing the U-Net model traditionally used in Latent Diffusion Models (LDMs) ( Citation: Rombach, Blattmann \u0026 al., 2022 Rombach, R., Blattmann, A., Lorenz, D., Esser, P. \u0026 Ommer, B. (2022). High-resolution image synthesis with latent diffusion models. IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 10684‚Äì‚Äì10695. ) with a transformer-based diffusion approach. In this article, we will explore what DiT is and why it has become a preferred choice for image generation.\nWhy DiT? The simple answer is: ‚Äúit delivers better results.‚Äù But when we dig deeper, two key reasons stand out:\nFlexibility: Transformers excel at generalizing across different modalities, making them highly versatile for generative tasks beyond image creation, such as text or audio generation. Additionally, the incorporation of the AdaLN (adaptive Layer Normalization) approach allows for the seamless integration of different input modalities, further enhancing the model‚Äôs adaptability.\nScalability: DiT shows a strong correlation between model complexity and image quality. This means that as the model size increases, the generated images‚Äô quality improves significantly. DiT scales efficiently, making it a powerful tool for generating high-resolution and detailed images.\nDiT Archiecture Illustration of the archiecture for DiT. Among the three archiectures, the authors adopt the one with AdaLN-zero which shows better result. Image is taken from ( Citation: Peebles \u0026 Xie, 2023 Peebles, W. \u0026 Xie, S. (2023). Scalable diffusion models with transformers. IEEE/CVF International Conference on Computer Vision (ICCV). ) Overview Similar to U-Net, DiT uses distinct blocks for both encoding (DiT blocks) and decoding (linear layers). Since transformers require input data to be in sequence format, the first step involves ‚Äúpatchifying‚Äù the image, converting it into a sequence. This sequence is then passed through several DiT blocks before being decoded by a linear layer.\nPatchifying Patchifying is a standard operation in ViT-based architectures to transform an image into a sequence. The core idea is to collapse the height and width dimensions of the image into a single sequence dimension. However, directly converting an entire image into a sequence would result in computationally expensive operations for subsequent layers. To mitigate this, the image is first divided into smaller patches.\nFormally, given a latent representation with dimensions \\(\\mathbb{R}^{C \\times H \\times W}\\) and a predefined patch size of \\(\\mathbb{R}^{p \\times p}\\), the patchified output would be \\(\\mathbb{R}^{d \\times (T_H \\cdot T_W)}\\), where \\(T_H = H/p\\) and \\(T_W = W/p\\). Typically, \\(T_H\\) and \\(T_W\\) are equal. It‚Äôs important to note that in this context, the input is referred to as ‚Äúlatent‚Äù because it is the output of the VAE model, rather than the original image. For instance, an image with dimensions \\(3 \\times 512 \\times 512\\) would be transformed to a latent of size \\(4 \\times 64 \\times 64\\) after passing through the VAE. Below is an example of the code used for patch embedding.\nCode for Patchifying class PatchEmbed(nn.Module): \"\"\"Adapted from https://github.com/pprp/timm/blob/master/timm/layers/patch_embed.py\"\"\" def __init__( self, img_size=(64, 64), patch_size=(2, 2), in_chans=4, embed_dim=768, ): super().__init__() self.img_size = img_size self.patch_size = patch_size self.grid_size = ( img_size[0] // patch_size[0], img_size[1] // patch_size[1] ) self.num_patches = self.grid_size[0] * self.grid_size[1] self.proj = nn.Conv2d( in_chans, embed_dim, kernel_size=patch_size, stride=patch_size ) def forward(self, x): # Shape of x: (B, C, H, W) x = self.proj(x) # Shape of x: (B, N, D), N = (HW/p^2) x = x.flatten(2).transpose(1, 2) return x Although patchifying is a standard operation for vision transformers, it plays a particularly crucial role in DiT. The patch size used in DiT is often smaller than that used in typical vision transformer models. For example, state-of-the-art models use a patch size of \\(2 \\times 2\\), whereas in image classification tasks, patch sizes are generally larger, such as \\(16 \\times 16\\) or \\(14 \\times 14\\). This difference is important because smaller patch sizes allow the model to capture finer details, which is critical for tasks like image generation where high granularity and attention to detail are essential.\nAdaLN-zero In the figure above, we see three different types of normalization. Here, we focus on the adopted version, AdaLN-zero. Before diving into the architectural design, let‚Äôs first define what AdaLN-zero is. Essentially, AdaLN-zero is a variant of Adaptive Layer Norm (AdaLN) with zero initialization. The concept of AdaLN was first introduced by ( Citation: Perez, Strub \u0026 al., 2018 Perez, E., Strub, F., De Vries, H., Dumoulin, V. \u0026 Courville, A. (2018). Film: Visual reasoning with a general conditioning layer. AAAI Conference on Artificial Intelligence, 32. ) . The key idea behind AdaLN is that it normalizes the input based on the conditional properties, rather than using fixed or learnable parameters shared across all inputs. More specifically, given an input \\(x \\in \\mathbb{R}^{N \\times d}\\), a conditional input \\( c \\in \\mathbb{R}^{N \\times d}\\) and two functions \\(f: \\mathbb{R}^{N \\times d} \\rightarrow \\mathbb{R}^{N \\times d}\\) and \\(g: \\mathbb{R}^{N \\times d} \\rightarrow \\mathbb{R}^{N \\times d}\\), AdaLN is defined as: $$ AdaLN(x) = f(c) * \\text{norm}(x) + g(c), $$ where: $$ \\text{norm}(x) = \\frac{x - \\text{mean}(x)}{\\text{std}(x) + \\text{eps}}. $$\nHere, \\(*\\), \\(-\\), and \\(+\\) are element-wise operations and \\(f\\) and \\(g\\) can be nueral networks. In simpler terms, AdaLN adjusts the shape of the distribution (via \\(f\\)) and shifts its position (via \\(g\\)) according to condition, thus allowing the model to cover a broader range of input variations.\nThe ‚Äúzero‚Äù in AdaLN-zero refers to initializing the weights for \\(f\\) and \\(g\\) to 0. This strategy, based on ( Citation: Goyal, 2017 Goyal, P. (2017). Accurate, large minibatch SG D: training imagenet in 1 hour. arXiv preprint arXiv:1706.02677. ) , accelerates training for large models in supervised learning settings. However, since this initialization could cause the output to always be 0, we modify the function by adding 1 to \\(f(x)\\). This ensures that the output of AdaLN-zero in the early stage is simply the normalized input:\n$$ AdaLN(x) = (1 + f(c)) * \\text{norm}(x) + g(c). $$\nThis modification allows the model to start with the standard normalization output while gradually learning to adjust as training progresses.\nCode for AdaLN-zero class AdaLN_zero(nn.Module): def __init__(self, hidden_size): super().__init__() # `elementwise_affine=False` means no learnable parameters self.norm = nn.LayerNorm(hidden_size, elementwise_affine=False, eps=1e-6) self.AdaLN_modulation = nn.Sequential( nn.SiLU(), nn.Linear(hidden_size, 2 * hidden_size), ) self.zero_initialization() def zero_initialization(self): nn.init.constant_(self.AdaLN_modulation[-1].weight, 0) nn.init.constant_(self.AdaLN_modulation[-1].bias, 0) def forward(self, x, c): # Shape of c: (B, D) # Shape of x: (B, N, D) shift, scale = self.AdaLN_modulation(c).chunk(2, dim=1) x = (1 + scale.unsqueeze(1)) * self.norm(x) + shift.unsqueeze(1) return x In DiT, the scale and shift are conditioned timestep and text input. This allows the normalization to adapt dynamically based on specific conditions, enhancing the model‚Äôs ability to handle diverse input scenarios.\nDiT Block The structure of the DiT block is quite similar to that of a Vision Transformer (ViT), featuring a self-attention layer followed by a feedforward network. However, DiT introduces two key modifications:\nAn AdaLN-zero layer is added before both the self-attention and feedforward layers, which helps adaptively normalize the inputs. Gating parameters are incorporated after the self-attention and feedforward layers to scale their outputs. The gating parameters are also generated by a neural network, but unlike AdaLN-zero, the weights for this network are not initialized to zero, allowing for more flexibility in scaling.\nCode for DiT Block class Mlp(nn.Module): \"\"\" Adapted from: https://github.com/huggingface/pytorch-image-models/blob/main/timm/layers/mlp.py \"\"\" def __init__( self, in_features, hidden_features, act_layer=nn.GELU, ): super().__init__() self.fc1 = nn.Linear(in_features, hidden_features) self.act = act_layer() self.fc2 = nn.Linear(hidden_features, in_features) def forward(self, x): x = self.fc2(self.act(self.fc1(x))) return x class DiTBlock(nn.Module): def __init__(self, hidden_size, num_heads, mlp_ratio=4.0, **block_kwargs): super().__init__() self.norm1 = nn.LayerNorm(hidden_size, elementwise_affine=False, eps=1e-6) self.attn = Attention(hidden_size, num_heads=num_heads, qkv_bias=True, **block_kwargs) self.norm2 = nn.LayerNorm(hidden_size, elementwise_affine=False, eps=1e-6) self.mlp = Mlp( in_features=hidden_size, hidden_features=int(hidden_size * mlp_ratio), act_layer=lambda: nn.GELU(approximate=\"tanh\"), ) self.AdaLN_modulation_1 = AdaLN_zero(hidden_size) self.AdaLN_modulation_2 = AdaLN_zero(hidden_size) self.gate_scale = nn.Sequential( nn.SiLU(), nn.Linear(hidden_size, hidden_size), ) def forward(self, x, c): # Shape of c: (B, D) # Shape of x: (B, N, D) gate_msa, gate_mlp = self.gate_scale(c).chunk(2, dim=1) x = x + gate_msa.unsqueeze(1) * self.attn(self.AdaLN_modulation_1(x, c)) x = x + gate_mlp.unsqueeze(1) * self.mlp(self.AdaLN_modulation_2(x, c)) return x Decoder In the original paper, the authors do not explicitly refer to the output layer as the ‚Äúdecoder.‚Äù However, for easier comparison with U-Net architectures, we refer to this final layer as the decoder. The decoder consists of a linear layer followed by an unpatchify operation. The linear layer maps the latent representation back to the original image size, and the unpatchify operation reconstructs the image from the sequence, reversing the patchification process.\nCode for the decoder class OutputLayer(nn.Module): \"\"\" The final layer of DiT. \"\"\" def __init__(self, hidden_size, patch_size, out_channels): super().__init__() self.norm_final = nn.LayerNorm( hidden_size, elementwise_affine=False, eps=1e-6, ) self.linear = nn.Linear( hidden_size, patch_size * patch_size * out_channels, ) self.AdaLN_modulation = AdaLN_zero(hidden_size) def forward(self, x, c): x = AdaLN_modulation(x) # Shape of x: (B, HW/p^2, D) x = self.linear(x) # Shape of x: (B, HW/p^2, p^2 * C) return x def unpatchify(x, patch_size: int = 2, out_channels: int = 3): \"\"\" x: (N, T, patch_size**2 * C) imgs: (N, H, W, C) \"\"\" height = width = int(x.shape[1] ** 0.5) x = x.reshape( x.shape[0], height, width, patch_size, patch_size, out_channels ) x = torch.einsum('nhwpqc-\u003enchpwq', x) imgs = x.reshape( x.shape[0], out_channels, height * patch_size, width * patch_size, ) return imgs With this, we have a complete overview of the DiT architecture. We highly recommend users check out the full implementation here.\nPixArt-\\(\\alpha\\) Training large diffusion models is computationally intensive, requiring vast resources. To reduce these costs while maintaining high performance, PixArt-\\(\\alpha\\) ( Citation: Chen, YU \u0026 al., 2024 Chen, J., YU, J., GE, C., Yao, L., Xie, E., Wang, Z., Kwok, J., Luo, P., Lu, H. \u0026 Li, Z. (2024). PixArt-(alpha): Fast Training of Diffusion Transformer for Photorealistic Text-to-Image Synthesis. International Conference on Learning Representations (ICLR). ) introduces an efficient training strategy, as illustrated in the figure below.\nImages from PixArt-\\(\\alpha\\) ( Citation: Chen, YU \u0026 al., 2024 Chen, J., YU, J., GE, C., Yao, L., Xie, E., Wang, Z., Kwok, J., Luo, P., Lu, H. \u0026 Li, Z. (2024). PixArt-(alpha): Fast Training of Diffusion Transformer for Photorealistic Text-to-Image Synthesis. International Conference on Learning Representations (ICLR). ) . (Left): Comparison of data usage and training time. (Right): Comparison of CO2 emissions and training cost.\nPixArt-\\(\\alpha\\) decomposes the training process into three stages and leverages a more efficient transformer architecture, enabling a significant boost in training efficiency without sacrificing model quality.\nDecomposing the Training Process The authors break the training process into three distinct stages:\nStage 1: Pixel Dependency Learning ‚Äì This stage focuses on learning class-conditional image generation, which is relatively straightforward and computationally inexpensive. Additionally, the model is initialized with pre-trained weights from ImageNet, further enhancing training efficiency.\nStage 2: Text-Image Alignment ‚Äì Here, the authors refine the alignment between text descriptions and images by cleaning the data. An efficient pipeline for automatic image-text pair labeling is introduced to ensure that the text descriptions are a better match for the images.\nStage 3: High-Resolution and Aesthetic Image Generation ‚Äì In this final stage, the model is trained to generate high-resolution images with improved quality. Since the model already has the capability to generate images by this stage, it converges quickly and efficiently.\nEfficient T2I Transformer Diffusion Transformer architecture from PixArt-\\(\\alpha\\) ( Citation: Chen, YU \u0026 al., 2024 Chen, J., YU, J., GE, C., Yao, L., Xie, E., Wang, Z., Kwok, J., Luo, P., Lu, H. \u0026 Li, Z. (2024). PixArt-(alpha): Fast Training of Diffusion Transformer for Photorealistic Text-to-Image Synthesis. International Conference on Learning Representations (ICLR). ) .\nThe transformer architecture in PixArt-\\(\\alpha\\) incorporates several optimizations to enhance training efficiency:\nCross-Attention Layer ‚Äì A cross-attention layer for text, similar to U-Net, is introduced. The AdaLN-zero layer handles the timestep processing, ensuring efficient temporal alignment.\nAdaLN-single ‚Äì Rather than using separate AdaLN layers for each block, all blocks share the same AdaLN-single layer. This simplification reduces the computational overhead while maintaining performance.\nQuick Start with Diffusers ü§ó The following example is to generate the image using the pretrained DiT model. Note that the code below is only limitted to ImageNet classes.\nfrom diffusers import DiTPipeline, DPMSolverMultistepScheduler import torch import numpy as np from PIL import Image pipe = DiTPipeline.from_pretrained(\"facebook/DiT-XL-2-512\", torch_dtype=torch.float16) pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config) pipe = pipe.to(\"cuda\") # Users should pick word from ImageNet classes # If users are not sure, please check with `pipe.labels` words = [\"African elephant\", \"gray wolf\", \"hotdog\", \"coffee mug\"] class_ids = pipe.get_label_ids(words) output = pipe(class_labels=class_ids, num_inference_steps=50) concat_images = [np.array(img) for img in output.images] concat_images = np.concatenate(concat_images, axis=1) # Save the generated image Image.fromarray(concat_images).save(\"target.png\") Another example is to use PixArt-\\(\\alpha\\) ( Citation: Chen, YU \u0026 al., 2024 Chen, J., YU, J., GE, C., Yao, L., Xie, E., Wang, Z., Kwok, J., Luo, P., Lu, H. \u0026 Li, Z. (2024). PixArt-(alpha): Fast Training of Diffusion Transformer for Photorealistic Text-to-Image Synthesis. International Conference on Learning Representations (ICLR). ) , which also adtops DiT model. The code below is to generate images using the pretrained PixArt-\\(\\alpha\\) model.\nfrom diffusers import PixArtAlphaPipeline import torch import numpy as np from PIL import Image pipe = PixArtAlphaPipeline.from_pretrained( \"PixArt-alpha/PixArt-XL-2-512x512\", torch_dtype=torch.bfloat16 ) pipe = pipe.to(\"cuda\") # With faster speed and lower memory usage pipe.enable_xformers_memory_efficient_attention() prompt = [ \"A dot is jumping\", \"A cute cat\", \"A fantasy landscape with a castle\", \"A happy family in a park\", ] output = pipe(prompt=prompt) concat_images = [np.array(img) for img in output.images] concat_images = np.concatenate(concat_images, axis=1) # Save the generated image Image.fromarray(concat_images).save(\"target.png\") Results generated with PixArt-\\(\\alpha\\) ( Citation: Chen, YU \u0026 al., 2024 Chen, J., YU, J., GE, C., Yao, L., Xie, E., Wang, Z., Kwok, J., Luo, P., Lu, H. \u0026 Li, Z. (2024). PixArt-(alpha): Fast Training of Diffusion Transformer for Photorealistic Text-to-Image Synthesis. International Conference on Learning Representations (ICLR). ) Conclusion DiT and PixArt-\\(\\alpha\\) represent significant advancements in the field of diffusion models and image generation. DiT, with its transformer-based architecture and innovative use of AdaLN-zero, brings flexibility, scalability, and improved generation quality. PixArt-\\(\\alpha\\) further enhances efficiency by breaking down the training process into stages and incorporating an optimized transformer architecture, reducing computational costs while maintaining high-resolution and aesthetic image generation. Together, these innovations pave the way for more powerful and efficient diffusion models in the future of AI-driven generative tasks.\nReference Peebles, W. \u0026 Xie, S. (2023). Scalable diffusion models with transformers. IEEE/CVF International Conference on Computer Vision (ICCV). Rombach, R., Blattmann, A., Lorenz, D., Esser, P. \u0026 Ommer, B. (2022). High-resolution image synthesis with latent diffusion models. IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 10684‚Äì‚Äì10695. Perez, E., Strub, F., De Vries, H., Dumoulin, V. \u0026 Courville, A. (2018). Film: Visual reasoning with a general conditioning layer. AAAI Conference on Artificial Intelligence, 32. Goyal, P. (2017). Accurate, large minibatch SG D: training imagenet in 1 hour. arXiv preprint arXiv:1706.02677. Chen, J., YU, J., GE, C., Yao, L., Xie, E., Wang, Z., Kwok, J., Luo, P., Lu, H. \u0026 Li, Z. (2024). PixArt-(alpha): Fast Training of Diffusion Transformer for Photorealistic Text-to-Image Synthesis. International Conference on Learning Representations (ICLR). ","wordCount":"2494","inLanguage":"en","image":"https://justin900429.github.io/images/generate.png","datePublished":"2024-09-18T18:45:59+08:00","dateModified":"2024-09-18T18:45:59+08:00","author":{"@type":"Person","name":"Justin"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://justin900429.github.io/en/posts/paper_reading/dit-intro/"},"publisher":{"@type":"Organization","name":"Justin's Site","logo":{"@type":"ImageObject","url":"https://justin900429.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://justin900429.github.io/en/ accesskey=h title="Justin's Site (Alt + H)">Justin's Site</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://justin900429.github.io/en/ title="üè† home"><span>üè† home</span></a></li><li><a href=https://justin900429.github.io/en/posts title="üìö article"><span>üìö article</span></a></li><li><a href=https://justin900429.github.io/en/search/ title="üîç search (Alt + /)" accesskey=/><span>üîç search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://justin900429.github.io/en/>Home</a>&nbsp;¬ª&nbsp;<a href=https://justin900429.github.io/en/posts/>üìö article</a>&nbsp;¬ª&nbsp;<a href=https://justin900429.github.io/en/posts/paper_reading/>üìñ paper-reading</a></div><h1 class="post-title entry-hint-parent">Diffusion Transformer: A powerful archiecture for image generation</h1><div class=post-meta><span title='2024-09-18 18:45:59 +0800 +0800'>September 18, 2024</span>&nbsp;¬∑&nbsp;12 min&nbsp;¬∑&nbsp;2494 words&nbsp;¬∑&nbsp;Justin</div></header><figure class=entry-cover><img loading=eager srcset="https://justin900429.github.io/en/posts/paper_reading/dit-intro/images/generate_hu718010be84836431c8834cf14491bbc0_1655957_360x0_resize_box_3.png 360w ,https://justin900429.github.io/en/posts/paper_reading/dit-intro/images/generate_hu718010be84836431c8834cf14491bbc0_1655957_480x0_resize_box_3.png 480w ,https://justin900429.github.io/en/posts/paper_reading/dit-intro/images/generate_hu718010be84836431c8834cf14491bbc0_1655957_720x0_resize_box_3.png 720w ,https://justin900429.github.io/en/posts/paper_reading/dit-intro/images/generate_hu718010be84836431c8834cf14491bbc0_1655957_1080x0_resize_box_3.png 1080w ,https://justin900429.github.io/en/posts/paper_reading/dit-intro/images/generate_hu718010be84836431c8834cf14491bbc0_1655957_1500x0_resize_box_3.png 1500w ,https://justin900429.github.io/en/posts/paper_reading/dit-intro/images/generate.png 2048w" sizes="(min-width: 768px) 720px, 100vw" src=https://justin900429.github.io/en/posts/paper_reading/dit-intro/images/generate.png alt="DiT output" width=2048 height=512><p>Generated image using DiT under 512x512 resolution.</p></figure><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#introduction aria-label=Introduction>Introduction</a><ul><li><a href=#why-dit aria-label="Why DiT?">Why DiT?</a></li></ul></li><li><a href=#dit-archiecture aria-label="DiT Archiecture">DiT Archiecture</a><ul><li><a href=#overview aria-label=Overview>Overview</a></li><li><a href=#patchifying aria-label=Patchifying>Patchifying</a></li><li><a href=#adaln-zero aria-label=AdaLN-zero>AdaLN-zero</a></li><li><a href=#dit-block aria-label="DiT Block">DiT Block</a></li><li><a href=#decoder aria-label=Decoder>Decoder</a></li></ul></li><li><a href=#pixart-alpha aria-label=PixArt-\(\alpha\)>PixArt-\(\alpha\)</a><ul><li><a href=#decomposing-the-training-process aria-label="Decomposing the Training Process">Decomposing the Training Process</a></li><li><a href=#efficient-t2i-transformer aria-label="Efficient T2I Transformer">Efficient T2I Transformer</a></li></ul></li><li><a href=#quick-start-with-diffusers- aria-label="Quick Start with Diffusers ü§ó">Quick Start with Diffusers ü§ó</a></li><li><a href=#conclusion aria-label=Conclusion>Conclusion</a></li><li><a href=#reference aria-label=Reference>Reference</a></li></ul></div></details></div><div class=post-content><h2 id=introduction>Introduction<a hidden class=anchor aria-hidden=true href=#introduction>#</a></h2><p>In recent developments, an increasing number of open-source models for image generation have adopted the Diffusion Transformer (DiT)
<span class=hugo-cite-intext itemprop=citation>(<span class=hugo-cite-group>
<a href=#peebles2023scalable><span class=visually-hidden>Citation: </span><span itemprop=author itemscope itemtype=https://schema.org/Person><meta itemprop=givenName content="William"><span itemprop=familyName>Peebles</span></span> & <span itemprop=author itemscope itemtype=https://schema.org/Person><meta itemprop=givenName content="Saining"><span itemprop=familyName>Xie</span></span>, <span itemprop=datePublished>2023</span></a><span class=hugo-cite-citation>
<span itemscope itemtype=https://schema.org/Article data-type=article><span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Peebles</span>, <meta itemprop=givenName content="William">W.</span> & <span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Xie</span>, <meta itemprop=givenName content="Saining">S.</span>
 
(<span itemprop=datePublished>2023</span>).
 <span itemprop=name>Scalable diffusion models with transformers</span>.<i>
<span itemprop=about>IEEE/CVF International Conference on Computer Vision (ICCV)</span></i>.</span>
</span></span>)</span>
as their backbone. Notable examples include <a href=https://stability.ai/news/stable-diffusion-3>Stable Diffusion 3 (SD3)</a>, <a href=https://openai.com/index/sora/>Sora</a>, <a href=https://github.com/hpcaitech/Open-Sora>Open-Sora</a>, and <a href=https://github.com/black-forest-labs/flux>FLUX</a>. These models demonstrate significant improvements in image quality over previous architectures. DiT represents an advancement by replacing the U-Net model traditionally used in Latent Diffusion Models (LDMs)
<span class=hugo-cite-intext itemprop=citation>(<span class=hugo-cite-group>
<a href=#rombach2022high><span class=visually-hidden>Citation: </span><span itemprop=author itemscope itemtype=https://schema.org/Person><meta itemprop=givenName content="Robin"><span itemprop=familyName>Rombach</span></span>, <span itemprop=author itemscope itemtype=https://schema.org/Person><meta itemprop=givenName content="Andreas"><span itemprop=familyName>Blattmann</span></span>
<em>& al.</em>, <span itemprop=datePublished>2022</span></a><span class=hugo-cite-citation>
<span itemscope itemtype=https://schema.org/Article data-type=article><span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Rombach</span>, <meta itemprop=givenName content="Robin">R.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Blattmann</span>, <meta itemprop=givenName content="Andreas">A.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Lorenz</span>, <meta itemprop=givenName content="Dominik">D.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Esser</span>, <meta itemprop=givenName content="Patrick">P.</span> & <span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Ommer</span>, <meta itemprop=givenName content="Bj√∂rn">B.</span>
 
(<span itemprop=datePublished>2022</span>).
 <span itemprop=name>High-resolution image synthesis with latent diffusion models</span>.<i>
<span itemprop=about>IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</span></i>. <span itemprop=pagination>10684‚Äì‚Äì10695</span>.</span>
</span></span>)</span>
with a transformer-based diffusion approach. In this article, we will explore what DiT is and why it has become a preferred choice for image generation.</p><h3 id=why-dit>Why DiT?<a hidden class=anchor aria-hidden=true href=#why-dit>#</a></h3><p>The simple answer is: ‚Äú<em>it delivers better results.</em>‚Äù But when we dig deeper, two key reasons stand out:</p><ol><li><p><strong>Flexibility</strong>: Transformers excel at generalizing across different modalities, making them highly versatile for generative tasks beyond image creation, such as text or audio generation. Additionally, the incorporation of the AdaLN (adaptive Layer Normalization) approach allows for the seamless integration of different input modalities, further enhancing the model&rsquo;s adaptability.</p></li><li><p><strong>Scalability</strong>: DiT shows a strong correlation between model complexity and image quality. This means that as the model size increases, the generated images&rsquo; quality improves significantly. DiT scales efficiently, making it a powerful tool for generating high-resolution and detailed images.</p></li></ol><h2 id=dit-archiecture>DiT Archiecture<a hidden class=anchor aria-hidden=true href=#dit-archiecture>#</a></h2><figure><img loading=lazy src=images/block.svg><figcaption><p>Illustration of the archiecture for DiT. Among the three archiectures, the authors adopt the one with AdaLN-zero which shows better result. Image is taken from
<span class=hugo-cite-intext itemprop=citation>(<span class=hugo-cite-group>
<a href=#peebles2023scalable><span class=visually-hidden>Citation: </span><span itemprop=author itemscope itemtype=https://schema.org/Person><meta itemprop=givenName content="William"><span itemprop=familyName>Peebles</span></span> & <span itemprop=author itemscope itemtype=https://schema.org/Person><meta itemprop=givenName content="Saining"><span itemprop=familyName>Xie</span></span>, <span itemprop=datePublished>2023</span></a><span class=hugo-cite-citation>
<span itemscope itemtype=https://schema.org/Article data-type=article><span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Peebles</span>, <meta itemprop=givenName content="William">W.</span> & <span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Xie</span>, <meta itemprop=givenName content="Saining">S.</span>
 
(<span itemprop=datePublished>2023</span>).
 <span itemprop=name>Scalable diffusion models with transformers</span>.<i>
<span itemprop=about>IEEE/CVF International Conference on Computer Vision (ICCV)</span></i>.</span>
</span></span>)</span></p></figcaption></figure><h3 id=overview>Overview<a hidden class=anchor aria-hidden=true href=#overview>#</a></h3><p>Similar to U-Net, DiT uses distinct blocks for both encoding (DiT blocks) and decoding (linear layers). Since transformers require input data to be in sequence format, the first step involves &ldquo;patchifying&rdquo; the image, converting it into a sequence. This sequence is then passed through several DiT blocks before being decoded by a linear layer.</p><h3 id=patchifying>Patchifying<a hidden class=anchor aria-hidden=true href=#patchifying>#</a></h3><p>Patchifying is a standard operation in ViT-based architectures to transform an image into a sequence. The core idea is to collapse the height and width dimensions of the image into a single sequence dimension. However, directly converting an entire image into a sequence would result in computationally expensive operations for subsequent layers. To mitigate this, the image is first divided into smaller patches.</p><p>Formally, given a latent representation with dimensions \(\mathbb{R}^{C \times H \times W}\) and a predefined patch size of \(\mathbb{R}^{p \times p}\), the patchified output would be \(\mathbb{R}^{d \times (T_H \cdot T_W)}\), where \(T_H = H/p\) and \(T_W = W/p\). Typically, \(T_H\) and \(T_W\) are equal. It&rsquo;s important to note that in this context, the input is referred to as &ldquo;latent&rdquo; because it is the output of the VAE model, rather than the original image. For instance, an image with dimensions \(3 \times 512 \times 512\) would be transformed to a latent of size \(4 \times 64 \times 64\) after passing through the VAE. Below is an example of the code used for patch embedding.</p><p><details><summary markdown=span><mark>Code for Patchifying</mark></summary><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>PatchEmbed</span>(nn<span style=color:#f92672>.</span>Module):
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34;Adapted from https://github.com/pprp/timm/blob/master/timm/layers/patch_embed.py&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> __init__(
</span></span><span style=display:flex><span>      self,
</span></span><span style=display:flex><span>      img_size<span style=color:#f92672>=</span>(<span style=color:#ae81ff>64</span>, <span style=color:#ae81ff>64</span>),
</span></span><span style=display:flex><span>      patch_size<span style=color:#f92672>=</span>(<span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>2</span>),
</span></span><span style=display:flex><span>      in_chans<span style=color:#f92672>=</span><span style=color:#ae81ff>4</span>,
</span></span><span style=display:flex><span>      embed_dim<span style=color:#f92672>=</span><span style=color:#ae81ff>768</span>,
</span></span><span style=display:flex><span>    ):
</span></span><span style=display:flex><span>        super()<span style=color:#f92672>.</span>__init__()
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>img_size <span style=color:#f92672>=</span> img_size
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>patch_size <span style=color:#f92672>=</span> patch_size
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>grid_size <span style=color:#f92672>=</span> (
</span></span><span style=display:flex><span>          img_size[<span style=color:#ae81ff>0</span>] <span style=color:#f92672>//</span> patch_size[<span style=color:#ae81ff>0</span>], 
</span></span><span style=display:flex><span>          img_size[<span style=color:#ae81ff>1</span>] <span style=color:#f92672>//</span> patch_size[<span style=color:#ae81ff>1</span>]
</span></span><span style=display:flex><span>        )
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>num_patches <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>grid_size[<span style=color:#ae81ff>0</span>] <span style=color:#f92672>*</span> self<span style=color:#f92672>.</span>grid_size[<span style=color:#ae81ff>1</span>]
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>proj <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Conv2d(
</span></span><span style=display:flex><span>          in_chans, embed_dim,
</span></span><span style=display:flex><span>          kernel_size<span style=color:#f92672>=</span>patch_size, stride<span style=color:#f92672>=</span>patch_size
</span></span><span style=display:flex><span>          )
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self, x):
</span></span><span style=display:flex><span>        <span style=color:#75715e># Shape of x: (B, C, H, W)</span>
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>proj(x)
</span></span><span style=display:flex><span>        <span style=color:#75715e># Shape of x: (B, N, D), N = (HW/p^2)</span>
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> x<span style=color:#f92672>.</span>flatten(<span style=color:#ae81ff>2</span>)<span style=color:#f92672>.</span>transpose(<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>2</span>)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> x
</span></span></code></pre></div></details></p><p>Although patchifying is a standard operation for vision transformers, it plays a particularly crucial role in DiT. The patch size used in DiT is often smaller than that used in typical vision transformer models. For example, state-of-the-art models use a patch size of \(2 \times 2\), whereas in image classification tasks, patch sizes are generally larger, such as \(16 \times 16\) or \(14 \times 14\). This difference is important because smaller patch sizes allow the model to capture finer details, which is critical for tasks like image generation where high granularity and attention to detail are essential.</p><h3 id=adaln-zero>AdaLN-zero<a hidden class=anchor aria-hidden=true href=#adaln-zero>#</a></h3><p>In the figure above, we see three different types of normalization. Here, we focus on the adopted version, <strong>AdaLN-zero</strong>. Before diving into the architectural design, let‚Äôs first define what AdaLN-zero is. Essentially, AdaLN-zero is a variant of Adaptive Layer Norm (<strong>AdaLN</strong>) with zero initialization. The concept of AdaLN was first introduced by
<span class=hugo-cite-intext itemprop=citation>(<span class=hugo-cite-group>
<a href=#perez2018film><span class=visually-hidden>Citation: </span><span itemprop=author itemscope itemtype=https://schema.org/Person><meta itemprop=givenName content="Ethan"><span itemprop=familyName>Perez</span></span>, <span itemprop=author itemscope itemtype=https://schema.org/Person><meta itemprop=givenName content="Florian"><span itemprop=familyName>Strub</span></span>
<em>& al.</em>, <span itemprop=datePublished>2018</span></a><span class=hugo-cite-citation>
<span itemscope itemtype=https://schema.org/Article data-type=article><span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Perez</span>, <meta itemprop=givenName content="Ethan">E.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Strub</span>, <meta itemprop=givenName content="Florian">F.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>De Vries</span>, <meta itemprop=givenName content="Harm">H.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Dumoulin</span>, <meta itemprop=givenName content="Vincent">V.</span> & <span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Courville</span>, <meta itemprop=givenName content="Aaron">A.</span>
 
(<span itemprop=datePublished>2018</span>).
 <span itemprop=name>Film: Visual reasoning with a general conditioning layer</span>.<i>
<span itemprop=about>AAAI Conference on Artificial Intelligence</span>, 32</i>.</span>
</span></span>)</span>
. The key idea behind AdaLN is that it normalizes the input based on the conditional properties, rather than using fixed or learnable parameters shared across all inputs. More specifically, given an input \(x \in \mathbb{R}^{N \times d}\), a conditional input \( c \in \mathbb{R}^{N \times d}\) and two functions \(f: \mathbb{R}^{N \times d} \rightarrow \mathbb{R}^{N \times d}\) and \(g: \mathbb{R}^{N \times d} \rightarrow \mathbb{R}^{N \times d}\), AdaLN is defined as:
$$
AdaLN(x) = f(c) * \text{norm}(x) + g(c),
$$
where:
$$
\text{norm}(x) = \frac{x - \text{mean}(x)}{\text{std}(x) + \text{eps}}.
$$</p><p>Here, \(*\), \(-\), and \(+\) are element-wise operations and \(f\) and \(g\) can be nueral networks. In simpler terms, AdaLN adjusts the shape of the distribution (via \(f\)) and shifts its position (via \(g\)) according to condition, thus allowing the model to cover a broader range of input variations.</p><p>The &ldquo;zero&rdquo; in AdaLN-zero refers to initializing the weights for \(f\) and \(g\) to 0. This strategy, based on
<span class=hugo-cite-intext itemprop=citation>(<span class=hugo-cite-group>
<a href=#goyal2017accurate><span class=visually-hidden>Citation: </span><span itemprop=author itemscope itemtype=https://schema.org/Person><meta itemprop=givenName content="P"><span itemprop=familyName>Goyal</span></span>, <span itemprop=datePublished>2017</span></a><span class=hugo-cite-citation>
<span itemscope itemtype=https://schema.org/Article data-type=article><span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Goyal</span>, <meta itemprop=givenName content="P">P.</span>
 
(<span itemprop=datePublished>2017</span>).
 <span itemprop=name>Accurate, large minibatch SG D: training imagenet in 1 hour</span>.<i>
<span itemprop=about>arXiv preprint arXiv:1706.02677</span></i>.</span>
</span></span>)</span>
, accelerates training for large models in supervised learning settings. However, since this initialization could cause the output to always be 0, we modify the function by adding 1 to \(f(x)\). This ensures that the output of AdaLN-zero in the early stage is simply the normalized input:</p><p>$$
AdaLN(x) = (1 + f(c)) * \text{norm}(x) + g(c).
$$</p><p>This modification allows the model to start with the standard normalization output while gradually learning to adjust as training progresses.</p><p><details><summary markdown=span><mark>Code for AdaLN-zero</mark></summary><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>AdaLN_zero</span>(nn<span style=color:#f92672>.</span>Module):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> __init__(self, hidden_size):
</span></span><span style=display:flex><span>        super()<span style=color:#f92672>.</span>__init__()
</span></span><span style=display:flex><span>        <span style=color:#75715e># `elementwise_affine=False` means no learnable parameters</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>norm <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>LayerNorm(hidden_size, elementwise_affine<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>, eps<span style=color:#f92672>=</span><span style=color:#ae81ff>1e-6</span>)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>AdaLN_modulation <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Sequential(
</span></span><span style=display:flex><span>            nn<span style=color:#f92672>.</span>SiLU(),
</span></span><span style=display:flex><span>            nn<span style=color:#f92672>.</span>Linear(hidden_size, <span style=color:#ae81ff>2</span> <span style=color:#f92672>*</span> hidden_size),
</span></span><span style=display:flex><span>        )
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>zero_initialization()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>zero_initialization</span>(self):
</span></span><span style=display:flex><span>        nn<span style=color:#f92672>.</span>init<span style=color:#f92672>.</span>constant_(self<span style=color:#f92672>.</span>AdaLN_modulation[<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>]<span style=color:#f92672>.</span>weight, <span style=color:#ae81ff>0</span>)
</span></span><span style=display:flex><span>        nn<span style=color:#f92672>.</span>init<span style=color:#f92672>.</span>constant_(self<span style=color:#f92672>.</span>AdaLN_modulation[<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>]<span style=color:#f92672>.</span>bias, <span style=color:#ae81ff>0</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self, x, c):
</span></span><span style=display:flex><span>        <span style=color:#75715e># Shape of c: (B, D)</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># Shape of x: (B, N, D)</span>
</span></span><span style=display:flex><span>        shift, scale <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>AdaLN_modulation(c)<span style=color:#f92672>.</span>chunk(<span style=color:#ae81ff>2</span>, dim<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> (<span style=color:#ae81ff>1</span> <span style=color:#f92672>+</span> scale<span style=color:#f92672>.</span>unsqueeze(<span style=color:#ae81ff>1</span>)) <span style=color:#f92672>*</span> self<span style=color:#f92672>.</span>norm(x) <span style=color:#f92672>+</span> shift<span style=color:#f92672>.</span>unsqueeze(<span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> x
</span></span></code></pre></div></details></p><p>In DiT, the <code>scale</code> and <code>shift</code> are conditioned timestep and text input. This allows the normalization to adapt dynamically based on specific conditions, enhancing the model‚Äôs ability to handle diverse input scenarios.</p><h3 id=dit-block>DiT Block<a hidden class=anchor aria-hidden=true href=#dit-block>#</a></h3><p>The structure of the DiT block is quite similar to that of a Vision Transformer (ViT), featuring a self-attention layer followed by a feedforward network. However, DiT introduces two key modifications:</p><ol><li>An <strong>AdaLN-zero</strong> layer is added before both the self-attention and feedforward layers, which helps adaptively normalize the inputs.</li><li>Gating parameters are incorporated after the self-attention and feedforward layers to scale their outputs.</li></ol><p>The gating parameters are also generated by a neural network, but unlike AdaLN-zero, the weights for this network are not initialized to zero, allowing for more flexibility in scaling.</p><p><details><summary markdown=span><mark>Code for DiT Block</mark></summary><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>Mlp</span>(nn<span style=color:#f92672>.</span>Module):
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34; 
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Adapted from: https://github.com/huggingface/pytorch-image-models/blob/main/timm/layers/mlp.py
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> __init__(
</span></span><span style=display:flex><span>            self,
</span></span><span style=display:flex><span>            in_features,
</span></span><span style=display:flex><span>            hidden_features,
</span></span><span style=display:flex><span>            act_layer<span style=color:#f92672>=</span>nn<span style=color:#f92672>.</span>GELU,
</span></span><span style=display:flex><span>    ):
</span></span><span style=display:flex><span>        super()<span style=color:#f92672>.</span>__init__()
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>fc1 <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Linear(in_features, hidden_features)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>act <span style=color:#f92672>=</span> act_layer()
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>fc2 <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Linear(hidden_features, in_features)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self, x):
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>fc2(self<span style=color:#f92672>.</span>act(self<span style=color:#f92672>.</span>fc1(x)))
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> x
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>DiTBlock</span>(nn<span style=color:#f92672>.</span>Module):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> __init__(self, hidden_size, num_heads, mlp_ratio<span style=color:#f92672>=</span><span style=color:#ae81ff>4.0</span>, <span style=color:#f92672>**</span>block_kwargs):
</span></span><span style=display:flex><span>        super()<span style=color:#f92672>.</span>__init__()
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>norm1 <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>LayerNorm(hidden_size, elementwise_affine<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>, eps<span style=color:#f92672>=</span><span style=color:#ae81ff>1e-6</span>)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>attn <span style=color:#f92672>=</span> Attention(hidden_size, num_heads<span style=color:#f92672>=</span>num_heads, qkv_bias<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>, <span style=color:#f92672>**</span>block_kwargs)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>norm2 <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>LayerNorm(hidden_size, elementwise_affine<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>, eps<span style=color:#f92672>=</span><span style=color:#ae81ff>1e-6</span>)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>mlp <span style=color:#f92672>=</span> Mlp(
</span></span><span style=display:flex><span>            in_features<span style=color:#f92672>=</span>hidden_size,
</span></span><span style=display:flex><span>            hidden_features<span style=color:#f92672>=</span>int(hidden_size <span style=color:#f92672>*</span> mlp_ratio), 
</span></span><span style=display:flex><span>            act_layer<span style=color:#f92672>=</span><span style=color:#66d9ef>lambda</span>: nn<span style=color:#f92672>.</span>GELU(approximate<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;tanh&#34;</span>),
</span></span><span style=display:flex><span>        )
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>AdaLN_modulation_1 <span style=color:#f92672>=</span> AdaLN_zero(hidden_size)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>AdaLN_modulation_2 <span style=color:#f92672>=</span> AdaLN_zero(hidden_size)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>gate_scale <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Sequential(
</span></span><span style=display:flex><span>          nn<span style=color:#f92672>.</span>SiLU(),
</span></span><span style=display:flex><span>          nn<span style=color:#f92672>.</span>Linear(hidden_size, hidden_size),
</span></span><span style=display:flex><span>        )
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self, x, c):
</span></span><span style=display:flex><span>        <span style=color:#75715e># Shape of c: (B, D)</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># Shape of x: (B, N, D)</span>
</span></span><span style=display:flex><span>        gate_msa, gate_mlp <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>gate_scale(c)<span style=color:#f92672>.</span>chunk(<span style=color:#ae81ff>2</span>, dim<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> x <span style=color:#f92672>+</span> gate_msa<span style=color:#f92672>.</span>unsqueeze(<span style=color:#ae81ff>1</span>) <span style=color:#f92672>*</span> self<span style=color:#f92672>.</span>attn(self<span style=color:#f92672>.</span>AdaLN_modulation_1(x, c))
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> x <span style=color:#f92672>+</span> gate_mlp<span style=color:#f92672>.</span>unsqueeze(<span style=color:#ae81ff>1</span>) <span style=color:#f92672>*</span> self<span style=color:#f92672>.</span>mlp(self<span style=color:#f92672>.</span>AdaLN_modulation_2(x, c))
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> x
</span></span></code></pre></div></details></p><h3 id=decoder>Decoder<a hidden class=anchor aria-hidden=true href=#decoder>#</a></h3><p>In the original paper, the authors do not explicitly refer to the output layer as the &ldquo;decoder.&rdquo; However, for easier comparison with U-Net architectures, we refer to this final layer as the decoder. The decoder consists of a linear layer followed by an <strong>unpatchify</strong> operation. The linear layer maps the latent representation back to the original image size, and the unpatchify operation reconstructs the image from the sequence, reversing the patchification process.</p><p><details><summary markdown=span><mark>Code for the decoder</mark></summary><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>OutputLayer</span>(nn<span style=color:#f92672>.</span>Module):
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    The final layer of DiT.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> __init__(self, hidden_size, patch_size, out_channels):
</span></span><span style=display:flex><span>        super()<span style=color:#f92672>.</span>__init__()
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>norm_final <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>LayerNorm(
</span></span><span style=display:flex><span>          hidden_size, 
</span></span><span style=display:flex><span>          elementwise_affine<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>, 
</span></span><span style=display:flex><span>          eps<span style=color:#f92672>=</span><span style=color:#ae81ff>1e-6</span>,
</span></span><span style=display:flex><span>        )
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>linear <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Linear(
</span></span><span style=display:flex><span>          hidden_size, 
</span></span><span style=display:flex><span>          patch_size <span style=color:#f92672>*</span> patch_size <span style=color:#f92672>*</span> out_channels,
</span></span><span style=display:flex><span>        )
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>AdaLN_modulation <span style=color:#f92672>=</span> AdaLN_zero(hidden_size)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self, x, c):
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> AdaLN_modulation(x)
</span></span><span style=display:flex><span>        <span style=color:#75715e># Shape of x: (B, HW/p^2, D)</span>
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>linear(x)
</span></span><span style=display:flex><span>        <span style=color:#75715e># Shape of x: (B, HW/p^2, p^2 * C)</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> x
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>unpatchify</span>(x, patch_size: int <span style=color:#f92672>=</span> <span style=color:#ae81ff>2</span>, out_channels: int <span style=color:#f92672>=</span> <span style=color:#ae81ff>3</span>):
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    x: (N, T, patch_size**2 * C)
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    imgs: (N, H, W, C)
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    height <span style=color:#f92672>=</span> width <span style=color:#f92672>=</span> int(x<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>1</span>] <span style=color:#f92672>**</span> <span style=color:#ae81ff>0.5</span>)
</span></span><span style=display:flex><span>    x <span style=color:#f92672>=</span> x<span style=color:#f92672>.</span>reshape(
</span></span><span style=display:flex><span>        x<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>0</span>], 
</span></span><span style=display:flex><span>        height,
</span></span><span style=display:flex><span>        width, 
</span></span><span style=display:flex><span>        patch_size, 
</span></span><span style=display:flex><span>        patch_size,
</span></span><span style=display:flex><span>        out_channels
</span></span><span style=display:flex><span>      )
</span></span><span style=display:flex><span>    x <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>einsum(<span style=color:#e6db74>&#39;nhwpqc-&gt;nchpwq&#39;</span>, x)
</span></span><span style=display:flex><span>    imgs <span style=color:#f92672>=</span> x<span style=color:#f92672>.</span>reshape(
</span></span><span style=display:flex><span>        x<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>0</span>], 
</span></span><span style=display:flex><span>        out_channels,  
</span></span><span style=display:flex><span>        height <span style=color:#f92672>*</span> patch_size,
</span></span><span style=display:flex><span>        width <span style=color:#f92672>*</span> patch_size,
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> imgs
</span></span></code></pre></div></details></p><p>With this, we have a complete overview of the DiT architecture. We highly recommend users check out the full implementation <a href=https://github.com/facebookresearch/DiT/blob/main/models.py>here</a>.</p><h2 id=pixart-alpha>PixArt-\(\alpha\)<a hidden class=anchor aria-hidden=true href=#pixart-alpha>#</a></h2><p>Training large diffusion models is computationally intensive, requiring vast resources. To reduce these costs while maintaining high performance, <strong>PixArt-\(\alpha\)</strong>
<span class=hugo-cite-intext itemprop=citation>(<span class=hugo-cite-group>
<a href=#chen2024pixartalpha><span class=visually-hidden>Citation: </span><span itemprop=author itemscope itemtype=https://schema.org/Person><meta itemprop=givenName content="Junsong"><span itemprop=familyName>Chen</span></span>, <span itemprop=author itemscope itemtype=https://schema.org/Person><meta itemprop=givenName content="Jincheng"><span itemprop=familyName>YU</span></span>
<em>& al.</em>, <span itemprop=datePublished>2024</span></a><span class=hugo-cite-citation>
<span itemscope itemtype=https://schema.org/Article data-type=article><span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Chen</span>, <meta itemprop=givenName content="Junsong">J.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>YU</span>, <meta itemprop=givenName content="Jincheng">J.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>GE</span>, <meta itemprop=givenName content="Chongjian">C.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Yao</span>, <meta itemprop=givenName content="Lewei">L.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Xie</span>, <meta itemprop=givenName content="Enze">E.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Wang</span>, <meta itemprop=givenName content="Zhongdao">Z.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Kwok</span>, <meta itemprop=givenName content="James">J.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Luo</span>, <meta itemprop=givenName content="Ping">P.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Lu</span>, <meta itemprop=givenName content="Huchuan">H.</span> & <span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Li</span>, <meta itemprop=givenName content="Zhenguo">Z.</span>
 
(<span itemprop=datePublished>2024</span>).
 <span itemprop=name>PixArt-(alpha): Fast Training of Diffusion Transformer for Photorealistic Text-to-Image Synthesis</span>.<i>
<span itemprop=about>International Conference on Learning Representations (ICLR)</span></i>.</span>
</span></span>)</span>
introduces an efficient training strategy, as illustrated in the figure below.</p><figure><img loading=lazy src=images/co2-dollar-lewei2.svg width=100%><figcaption><p>Images from PixArt-\(\alpha\)
<span class=hugo-cite-intext itemprop=citation>(<span class=hugo-cite-group>
<a href=#chen2024pixartalpha><span class=visually-hidden>Citation: </span><span itemprop=author itemscope itemtype=https://schema.org/Person><meta itemprop=givenName content="Junsong"><span itemprop=familyName>Chen</span></span>, <span itemprop=author itemscope itemtype=https://schema.org/Person><meta itemprop=givenName content="Jincheng"><span itemprop=familyName>YU</span></span>
<em>& al.</em>, <span itemprop=datePublished>2024</span></a><span class=hugo-cite-citation>
<span itemscope itemtype=https://schema.org/Article data-type=article><span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Chen</span>, <meta itemprop=givenName content="Junsong">J.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>YU</span>, <meta itemprop=givenName content="Jincheng">J.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>GE</span>, <meta itemprop=givenName content="Chongjian">C.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Yao</span>, <meta itemprop=givenName content="Lewei">L.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Xie</span>, <meta itemprop=givenName content="Enze">E.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Wang</span>, <meta itemprop=givenName content="Zhongdao">Z.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Kwok</span>, <meta itemprop=givenName content="James">J.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Luo</span>, <meta itemprop=givenName content="Ping">P.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Lu</span>, <meta itemprop=givenName content="Huchuan">H.</span> & <span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Li</span>, <meta itemprop=givenName content="Zhenguo">Z.</span>
 
(<span itemprop=datePublished>2024</span>).
 <span itemprop=name>PixArt-(alpha): Fast Training of Diffusion Transformer for Photorealistic Text-to-Image Synthesis</span>.<i>
<span itemprop=about>International Conference on Learning Representations (ICLR)</span></i>.</span>
</span></span>)</span>
. <strong>(Left)</strong>: Comparison of data usage and training time. <strong>(Right)</strong>: Comparison of <i>CO<sub>2</sub></i> emissions and training cost.</p></figcaption></figure><p>PixArt-\(\alpha\) decomposes the training process into three stages and leverages a more efficient transformer architecture, enabling a significant boost in training efficiency without sacrificing model quality.</p><h3 id=decomposing-the-training-process>Decomposing the Training Process<a hidden class=anchor aria-hidden=true href=#decomposing-the-training-process>#</a></h3><p>The authors break the training process into three distinct stages:</p><ol><li><p><strong>Stage 1: Pixel Dependency Learning</strong> ‚Äì This stage focuses on learning class-conditional image generation, which is relatively straightforward and computationally inexpensive. Additionally, the model is initialized with pre-trained weights from ImageNet, further enhancing training efficiency.</p></li><li><p><strong>Stage 2: Text-Image Alignment</strong> ‚Äì Here, the authors refine the alignment between text descriptions and images by cleaning the data. An efficient pipeline for automatic image-text pair labeling is introduced to ensure that the text descriptions are a better match for the images.</p></li><li><p><strong>Stage 3: High-Resolution and Aesthetic Image Generation</strong> ‚Äì In this final stage, the model is trained to generate high-resolution images with improved quality. Since the model already has the capability to generate images by this stage, it converges quickly and efficiently.</p></li></ol><h3 id=efficient-t2i-transformer>Efficient T2I Transformer<a hidden class=anchor aria-hidden=true href=#efficient-t2i-transformer>#</a></h3><figure><img loading=lazy src=images/pixart.svg style=margin-left:auto;margin-right:auto><figcaption><p style=text-align:center>Diffusion Transformer architecture from PixArt-\(\alpha\)
<span class=hugo-cite-intext itemprop=citation>(<span class=hugo-cite-group>
<a href=#chen2024pixartalpha><span class=visually-hidden>Citation: </span><span itemprop=author itemscope itemtype=https://schema.org/Person><meta itemprop=givenName content="Junsong"><span itemprop=familyName>Chen</span></span>, <span itemprop=author itemscope itemtype=https://schema.org/Person><meta itemprop=givenName content="Jincheng"><span itemprop=familyName>YU</span></span>
<em>& al.</em>, <span itemprop=datePublished>2024</span></a><span class=hugo-cite-citation>
<span itemscope itemtype=https://schema.org/Article data-type=article><span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Chen</span>, <meta itemprop=givenName content="Junsong">J.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>YU</span>, <meta itemprop=givenName content="Jincheng">J.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>GE</span>, <meta itemprop=givenName content="Chongjian">C.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Yao</span>, <meta itemprop=givenName content="Lewei">L.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Xie</span>, <meta itemprop=givenName content="Enze">E.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Wang</span>, <meta itemprop=givenName content="Zhongdao">Z.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Kwok</span>, <meta itemprop=givenName content="James">J.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Luo</span>, <meta itemprop=givenName content="Ping">P.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Lu</span>, <meta itemprop=givenName content="Huchuan">H.</span> & <span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Li</span>, <meta itemprop=givenName content="Zhenguo">Z.</span>
 
(<span itemprop=datePublished>2024</span>).
 <span itemprop=name>PixArt-(alpha): Fast Training of Diffusion Transformer for Photorealistic Text-to-Image Synthesis</span>.<i>
<span itemprop=about>International Conference on Learning Representations (ICLR)</span></i>.</span>
</span></span>)</span>
.</p></figcaption></figure><p>The transformer architecture in PixArt-\(\alpha\) incorporates several optimizations to enhance training efficiency:</p><ol><li><p><strong>Cross-Attention Layer</strong> ‚Äì A cross-attention layer for text, similar to U-Net, is introduced. The <strong>AdaLN-zero</strong> layer handles the timestep processing, ensuring efficient temporal alignment.</p></li><li><p><strong>AdaLN-single</strong> ‚Äì Rather than using separate AdaLN layers for each block, all blocks share the same <strong>AdaLN-single</strong> layer. This simplification reduces the computational overhead while maintaining performance.</p></li></ol><h2 id=quick-start-with-diffusers->Quick Start with Diffusers ü§ó<a hidden class=anchor aria-hidden=true href=#quick-start-with-diffusers->#</a></h2><p>The following example is to generate the image using the pretrained DiT model. Note that the code below is only limitted to ImageNet classes.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> diffusers <span style=color:#f92672>import</span> DiTPipeline, DPMSolverMultistepScheduler
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> torch
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> numpy <span style=color:#66d9ef>as</span> np
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> PIL <span style=color:#f92672>import</span> Image
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>pipe <span style=color:#f92672>=</span> DiTPipeline<span style=color:#f92672>.</span>from_pretrained(<span style=color:#e6db74>&#34;facebook/DiT-XL-2-512&#34;</span>, torch_dtype<span style=color:#f92672>=</span>torch<span style=color:#f92672>.</span>float16)
</span></span><span style=display:flex><span>pipe<span style=color:#f92672>.</span>scheduler <span style=color:#f92672>=</span> DPMSolverMultistepScheduler<span style=color:#f92672>.</span>from_config(pipe<span style=color:#f92672>.</span>scheduler<span style=color:#f92672>.</span>config)
</span></span><span style=display:flex><span>pipe <span style=color:#f92672>=</span> pipe<span style=color:#f92672>.</span>to(<span style=color:#e6db74>&#34;cuda&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Users should pick word from ImageNet classes</span>
</span></span><span style=display:flex><span><span style=color:#75715e># If users are not sure, please check with `pipe.labels`</span>
</span></span><span style=display:flex><span>words <span style=color:#f92672>=</span> [<span style=color:#e6db74>&#34;African elephant&#34;</span>,  <span style=color:#e6db74>&#34;gray wolf&#34;</span>, <span style=color:#e6db74>&#34;hotdog&#34;</span>, <span style=color:#e6db74>&#34;coffee mug&#34;</span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>class_ids <span style=color:#f92672>=</span> pipe<span style=color:#f92672>.</span>get_label_ids(words)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>output <span style=color:#f92672>=</span> pipe(class_labels<span style=color:#f92672>=</span>class_ids, num_inference_steps<span style=color:#f92672>=</span><span style=color:#ae81ff>50</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>concat_images <span style=color:#f92672>=</span> [np<span style=color:#f92672>.</span>array(img) <span style=color:#66d9ef>for</span> img <span style=color:#f92672>in</span> output<span style=color:#f92672>.</span>images]
</span></span><span style=display:flex><span>concat_images <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>concatenate(concat_images, axis<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Save the generated image</span>
</span></span><span style=display:flex><span>Image<span style=color:#f92672>.</span>fromarray(concat_images)<span style=color:#f92672>.</span>save(<span style=color:#e6db74>&#34;target.png&#34;</span>) 
</span></span></code></pre></div><p>Another example is to use PixArt-\(\alpha\)
<span class=hugo-cite-intext itemprop=citation>(<span class=hugo-cite-group>
<a href=#chen2024pixartalpha><span class=visually-hidden>Citation: </span><span itemprop=author itemscope itemtype=https://schema.org/Person><meta itemprop=givenName content="Junsong"><span itemprop=familyName>Chen</span></span>, <span itemprop=author itemscope itemtype=https://schema.org/Person><meta itemprop=givenName content="Jincheng"><span itemprop=familyName>YU</span></span>
<em>& al.</em>, <span itemprop=datePublished>2024</span></a><span class=hugo-cite-citation>
<span itemscope itemtype=https://schema.org/Article data-type=article><span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Chen</span>, <meta itemprop=givenName content="Junsong">J.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>YU</span>, <meta itemprop=givenName content="Jincheng">J.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>GE</span>, <meta itemprop=givenName content="Chongjian">C.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Yao</span>, <meta itemprop=givenName content="Lewei">L.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Xie</span>, <meta itemprop=givenName content="Enze">E.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Wang</span>, <meta itemprop=givenName content="Zhongdao">Z.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Kwok</span>, <meta itemprop=givenName content="James">J.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Luo</span>, <meta itemprop=givenName content="Ping">P.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Lu</span>, <meta itemprop=givenName content="Huchuan">H.</span> & <span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Li</span>, <meta itemprop=givenName content="Zhenguo">Z.</span>
 
(<span itemprop=datePublished>2024</span>).
 <span itemprop=name>PixArt-(alpha): Fast Training of Diffusion Transformer for Photorealistic Text-to-Image Synthesis</span>.<i>
<span itemprop=about>International Conference on Learning Representations (ICLR)</span></i>.</span>
</span></span>)</span>
, which also adtops DiT model. The code below is to generate images using the pretrained PixArt-\(\alpha\) model.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> diffusers <span style=color:#f92672>import</span> PixArtAlphaPipeline
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> torch
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> numpy <span style=color:#66d9ef>as</span> np
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> PIL <span style=color:#f92672>import</span> Image
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>pipe <span style=color:#f92672>=</span> PixArtAlphaPipeline<span style=color:#f92672>.</span>from_pretrained(
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;PixArt-alpha/PixArt-XL-2-512x512&#34;</span>, 
</span></span><span style=display:flex><span>    torch_dtype<span style=color:#f92672>=</span>torch<span style=color:#f92672>.</span>bfloat16
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>pipe <span style=color:#f92672>=</span> pipe<span style=color:#f92672>.</span>to(<span style=color:#e6db74>&#34;cuda&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># With faster speed and lower memory usage</span>
</span></span><span style=display:flex><span>pipe<span style=color:#f92672>.</span>enable_xformers_memory_efficient_attention()
</span></span><span style=display:flex><span>prompt <span style=color:#f92672>=</span> [
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;A dot is jumping&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;A cute cat&#34;</span>, 
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;A fantasy landscape with a castle&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;A happy family in a park&#34;</span>,
</span></span><span style=display:flex><span>]
</span></span><span style=display:flex><span>output <span style=color:#f92672>=</span> pipe(prompt<span style=color:#f92672>=</span>prompt)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>concat_images <span style=color:#f92672>=</span> [np<span style=color:#f92672>.</span>array(img) <span style=color:#66d9ef>for</span> img <span style=color:#f92672>in</span> output<span style=color:#f92672>.</span>images]
</span></span><span style=display:flex><span>concat_images <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>concatenate(concat_images, axis<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Save the generated image</span>
</span></span><span style=display:flex><span>Image<span style=color:#f92672>.</span>fromarray(concat_images)<span style=color:#f92672>.</span>save(<span style=color:#e6db74>&#34;target.png&#34;</span>) 
</span></span></code></pre></div><figure><img loading=lazy src=images/pixart_alpha.png><figcaption><p>Results generated with PixArt-\(\alpha\)
<span class=hugo-cite-intext itemprop=citation>(<span class=hugo-cite-group>
<a href=#chen2024pixartalpha><span class=visually-hidden>Citation: </span><span itemprop=author itemscope itemtype=https://schema.org/Person><meta itemprop=givenName content="Junsong"><span itemprop=familyName>Chen</span></span>, <span itemprop=author itemscope itemtype=https://schema.org/Person><meta itemprop=givenName content="Jincheng"><span itemprop=familyName>YU</span></span>
<em>& al.</em>, <span itemprop=datePublished>2024</span></a><span class=hugo-cite-citation>
<span itemscope itemtype=https://schema.org/Article data-type=article><span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Chen</span>, <meta itemprop=givenName content="Junsong">J.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>YU</span>, <meta itemprop=givenName content="Jincheng">J.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>GE</span>, <meta itemprop=givenName content="Chongjian">C.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Yao</span>, <meta itemprop=givenName content="Lewei">L.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Xie</span>, <meta itemprop=givenName content="Enze">E.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Wang</span>, <meta itemprop=givenName content="Zhongdao">Z.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Kwok</span>, <meta itemprop=givenName content="James">J.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Luo</span>, <meta itemprop=givenName content="Ping">P.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Lu</span>, <meta itemprop=givenName content="Huchuan">H.</span> & <span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Li</span>, <meta itemprop=givenName content="Zhenguo">Z.</span>
 
(<span itemprop=datePublished>2024</span>).
 <span itemprop=name>PixArt-(alpha): Fast Training of Diffusion Transformer for Photorealistic Text-to-Image Synthesis</span>.<i>
<span itemprop=about>International Conference on Learning Representations (ICLR)</span></i>.</span>
</span></span>)</span></p></figcaption></figure><h2 id=conclusion>Conclusion<a hidden class=anchor aria-hidden=true href=#conclusion>#</a></h2><p>DiT and PixArt-\(\alpha\) represent significant advancements in the field of diffusion models and image generation. DiT, with its transformer-based architecture and innovative use of AdaLN-zero, brings flexibility, scalability, and improved generation quality. PixArt-\(\alpha\) further enhances efficiency by breaking down the training process into stages and incorporating an optimized transformer architecture, reducing computational costs while maintaining high-resolution and aesthetic image generation. Together, these innovations pave the way for more powerful and efficient diffusion models in the future of AI-driven generative tasks.</p><h2 id=reference>Reference<a hidden class=anchor aria-hidden=true href=#reference>#</a></h2><section class=hugo-cite-bibliography><dl><ol><div id=peebles2023scalable><li><dd><span itemscope itemtype=https://schema.org/Article data-type=article><span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Peebles</span>, <meta itemprop=givenName content="William">W.</span> & <span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Xie</span>, <meta itemprop=givenName content="Saining">S.</span>
 
(<span itemprop=datePublished>2023</span>).
 <span itemprop=name>Scalable diffusion models with transformers</span>.<i>
<span itemprop=about>IEEE/CVF International Conference on Computer Vision (ICCV)</span></i>.</span></dd></li></div><div id=rombach2022high><li><dd><span itemscope itemtype=https://schema.org/Article data-type=article><span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Rombach</span>, <meta itemprop=givenName content="Robin">R.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Blattmann</span>, <meta itemprop=givenName content="Andreas">A.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Lorenz</span>, <meta itemprop=givenName content="Dominik">D.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Esser</span>, <meta itemprop=givenName content="Patrick">P.</span> & <span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Ommer</span>, <meta itemprop=givenName content="Bj√∂rn">B.</span>
 
(<span itemprop=datePublished>2022</span>).
 <span itemprop=name>High-resolution image synthesis with latent diffusion models</span>.<i>
<span itemprop=about>IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</span></i>. <span itemprop=pagination>10684‚Äì‚Äì10695</span>.</span></dd></li></div><div id=perez2018film><li><dd><span itemscope itemtype=https://schema.org/Article data-type=article><span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Perez</span>, <meta itemprop=givenName content="Ethan">E.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Strub</span>, <meta itemprop=givenName content="Florian">F.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>De Vries</span>, <meta itemprop=givenName content="Harm">H.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Dumoulin</span>, <meta itemprop=givenName content="Vincent">V.</span> & <span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Courville</span>, <meta itemprop=givenName content="Aaron">A.</span>
 
(<span itemprop=datePublished>2018</span>).
 <span itemprop=name>Film: Visual reasoning with a general conditioning layer</span>.<i>
<span itemprop=about>AAAI Conference on Artificial Intelligence</span>, 32</i>.</span></dd></li></div><div id=goyal2017accurate><li><dd><span itemscope itemtype=https://schema.org/Article data-type=article><span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Goyal</span>, <meta itemprop=givenName content="P">P.</span>
 
(<span itemprop=datePublished>2017</span>).
 <span itemprop=name>Accurate, large minibatch SG D: training imagenet in 1 hour</span>.<i>
<span itemprop=about>arXiv preprint arXiv:1706.02677</span></i>.</span></dd></li></div><div id=chen2024pixartalpha><li><dd><span itemscope itemtype=https://schema.org/Article data-type=article><span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Chen</span>, <meta itemprop=givenName content="Junsong">J.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>YU</span>, <meta itemprop=givenName content="Jincheng">J.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>GE</span>, <meta itemprop=givenName content="Chongjian">C.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Yao</span>, <meta itemprop=givenName content="Lewei">L.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Xie</span>, <meta itemprop=givenName content="Enze">E.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Wang</span>, <meta itemprop=givenName content="Zhongdao">Z.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Kwok</span>, <meta itemprop=givenName content="James">J.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Luo</span>, <meta itemprop=givenName content="Ping">P.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Lu</span>, <meta itemprop=givenName content="Huchuan">H.</span> & <span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Li</span>, <meta itemprop=givenName content="Zhenguo">Z.</span>
 
(<span itemprop=datePublished>2024</span>).
 <span itemprop=name>PixArt-(alpha): Fast Training of Diffusion Transformer for Photorealistic Text-to-Image Synthesis</span>.<i>
<span itemprop=about>International Conference on Learning Representations (ICLR)</span></i>.</span></dd></li></div></ol></dl></section></div><footer class=post-footer><ul class=post-tags></ul><nav class=paginav><a class=next href=https://justin900429.github.io/en/posts/coding/einops/><span class=title>Next ¬ª</span><br><span>Powerful Tools for Tensor Operations - Einops</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2024 <a href=https://justin900429.github.io/en/>Justin's Site</a></span> ¬∑
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>