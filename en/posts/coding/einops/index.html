<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Powerful Tools for Tensor Operations - Einops | Justin's Site</title><meta name=keywords content="PyTorch,Einops,Tensor operations,Deep learning"><meta name=description content="An introduction to Einops, a powerful tool for tensor operations."><meta name=author content="Justin"><link rel=canonical href=https://justin900429.github.io/en/posts/coding/einops/><link crossorigin=anonymous href=/assets/css/stylesheet.0fd896da64c856c1d033709781aee14ceeedda323fa9830eaf98bbabf131b313.css integrity="sha256-D9iW2mTIVsHQM3CXga7hTO7t2jI/qYMOr5i7q/ExsxM=" rel="preload stylesheet" as=style><link rel=icon href=https://justin900429.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://justin900429.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://justin900429.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://justin900429.github.io/apple-touch-icon.png><link rel=mask-icon href=https://justin900429.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css integrity=sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js integrity=sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4 crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js integrity=sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa crossorigin=anonymous onload=renderMathInElement(document.body)></script><meta property="og:title" content="Powerful Tools for Tensor Operations - Einops"><meta property="og:description" content="An introduction to Einops, a powerful tool for tensor operations."><meta property="og:type" content="article"><meta property="og:url" content="https://justin900429.github.io/en/posts/coding/einops/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2021-07-15T15:50:37+08:00"><meta property="article:modified_time" content="2021-07-15T15:50:37+08:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Powerful Tools for Tensor Operations - Einops"><meta name=twitter:description content="An introduction to Einops, a powerful tool for tensor operations."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"üìö article","item":"https://justin900429.github.io/en/posts/"},{"@type":"ListItem","position":2,"name":"üë®üèª‚Äçüíª coding","item":"https://justin900429.github.io/en/posts/coding/"},{"@type":"ListItem","position":3,"name":"Powerful Tools for Tensor Operations - Einops","item":"https://justin900429.github.io/en/posts/coding/einops/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Powerful Tools for Tensor Operations - Einops","name":"Powerful Tools for Tensor Operations - Einops","description":"An introduction to Einops, a powerful tool for tensor operations.","keywords":["PyTorch","Einops","Tensor operations","Deep learning"],"articleBody":"Introduction Einops is a powerful tool for tensor operations, including reduce, rearrange, repeat, and more. Its most impressive feature is its ability to make code more readable and reliable. Additionally, it includes nn.Module functionality, which is compatible with PyTorch for directly modifying tensor shapes.\nIn this tutorial, we‚Äôll use the MNIST dataset for demonstration purposes.\nInstallation To install the stable version of Einops, use pip:\npip install einops For the latest version, you can install it directly from the GitHub repository:\npip install git+https://github.com/arogozhnikov/einops Operations - Rearrange Changing Axes The rearrange function allows you to change the shape of a tensor. For instance, treating an image as a matrix, you can perform a transpose operation. If the original shape of the tensor is (h, w), transposing changes it to (w, h). With Python, you can write:\n# Shape of data: (h, w) transpose_data = data.transpose(0, 1) print(transpose_data.shape) \u003e\u003e torch.Size([w, h]) While this code is straightforward, it doesn‚Äôt clearly indicate the shape transformations. Using Einops, the same operation is more intuitive:\nfrom einops import rearrange data = rearrange(data, \"h w -\u003e w h\") Here, the transformation is described explicitly: \"h w -\u003e w h\". This syntax is clearer and easier to understand.\nFlattening Flattening combines multiple axes into a single dimension. For example, consider a tensor with shape (batch_size, width, height):\n# Shape of data: (batch_size, width, height) # Shape of stack_data: (batch_size * width, height) stack_data = rearrange(data, \"b h w -\u003e (b h) w\") You can also rearrange dimensions in a non-sequential order:\nstack_data = rearrange(data, \"b h w -\u003e h (b w)\") Output examples:\nCombining the batch and width dimensions. Combining the batch and height dimensions. Unflattening In addition to combining axes, you can split an axis into multiple dimensions:\ndecompose_data = rearrange(data, \"(b1 b2) h w -\u003e (b1 h) (b2 w)\", b1=2) Here, the batch size of 4 is split into two dimensions: b1=2 and b2=2.\nOperations - Reduce The reduce function simplifies tensors by reducing dimensions. It shares the same syntax as rearrange but allows changes in dimensionality. For instance, consider a tensor with shape (batch_size, height * h1, width * w1).\nMean Averaging Mean averaging reduces image size by averaging pixel values:\nreduce_img = reduce(data, \"b (h h1) (w w1) -\u003e h (b w)\", \"mean\", h1=2, w1=2) # Check the shape print(f\"Before: {data.shape}\") print(f\"After: {reduce_img.shape}\") \u003e\u003e\u003e Before: torch.Size([4, 28, 28]) \u003e\u003e\u003e After: torch.Size([14, 56]) Output:\nMax Pooling Similarly, you can apply max pooling to reduce size:\nreduce_img = reduce(data, \"b (h h1) (w w1) -\u003e h (b w)\", \"max\", h1=2, w1=2) # Check the shape print(f\"Before: {data.shape}\") print(f\"After: {reduce_img.shape}\") \u003e\u003e\u003e Before: torch.Size([4, 28, 28]) \u003e\u003e\u003e After: torch.Size([14, 56]) Operations - Repeat While reduce decreases dimensions, repeat increases them. For example, to expand an image tensor from (b, h, w) to (b, h, w, 3):\n# Shape of data: (batch_size, width, height) repeat_data = repeat(data, \"b h w -\u003e b h w c\", c=3) print(repeat_data.shape) \u003e\u003e torch.Size([4, 28, 28, 3]) Duplicating dimensions can also be done easily:\nrepeat_data = repeat(data, \"b h w -\u003e (b h) (w1 w)\", w1=3) PyTorch Layers Einops operations can be integrated into PyTorch layers, making them ideal for use in models:\nfrom einops.layers.torch import Rearrange model = nn.Sequential( nn.Conv2d(...), nn.Conv2d(...), ... Rearrange(\"b c h w -\u003e b (c h w)\") ... ) Application - Attention Einops simplifies implementing the attention mechanism, which calculates input weights based on dot products of queries and keys. Below is an example implementation:\nimport torch import torch.nn as nn from einops import rearrange class Attention(nn.Module): def __init__(self, n_head: int, d_in: int, d_model: int, dropout=0.1): super().__init__() self.n_head = n_head assert d_model % n_head == 0 self.q = nn.Linear(d_in, d_model) self.k = nn.Linear(d_in, d_model) self.v = nn.Linear(d_in, d_model) self.out = nn.Linear(d_model, d_in) self.dropout = nn.Dropout(p=dropout) self.layer_norm = nn.LayerNorm(d_model) def forward(self, x, mask=None): q = rearrange(self.q(x), 'b l (h q) -\u003e h b l q', h=self.n_head) k = rearrange(self.k(x), 'b t (h q) -\u003e h b t q', h=self.n_head) v = rearrange(self.v(x), 'b t (h v) -\u003e h b t v', h=self.n_head) attn = torch.einsum('hblq,hbtq-\u003ehblt', [q, k]) / q.shape[-1] ** 0.5 if mask is not None: attn = attn.masked_fill(mask[None], float('-inf')) attn = torch.softmax(attn, dim=3) output = torch.einsum('hblt,hbtv-\u003ehblv', [attn, v]) output = rearrange(output, 'h b l v -\u003e b l (h v)') output = self.dropout(self.out(output)) output = self.layer_norm(output + x) return output References Einops Documentation ","wordCount":"725","inLanguage":"en","datePublished":"2021-07-15T15:50:37+08:00","dateModified":"2021-07-15T15:50:37+08:00","author":{"@type":"Person","name":"Justin"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://justin900429.github.io/en/posts/coding/einops/"},"publisher":{"@type":"Organization","name":"Justin's Site","logo":{"@type":"ImageObject","url":"https://justin900429.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://justin900429.github.io/en/ accesskey=h title="Justin's Site (Alt + H)">Justin's Site</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://justin900429.github.io/en/ title="üè† home"><span>üè† home</span></a></li><li><a href=https://justin900429.github.io/en/posts title="üìö article"><span>üìö article</span></a></li><li><a href=https://justin900429.github.io/en/search/ title="üîç search (Alt + /)" accesskey=/><span>üîç search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://justin900429.github.io/en/>Home</a>&nbsp;¬ª&nbsp;<a href=https://justin900429.github.io/en/posts/>üìö article</a>&nbsp;¬ª&nbsp;<a href=https://justin900429.github.io/en/posts/coding/>üë®üèª‚Äçüíª coding</a></div><h1 class="post-title entry-hint-parent">Powerful Tools for Tensor Operations - Einops</h1><div class=post-meta><span title='2021-07-15 15:50:37 +0800 +0800'>July 15, 2021</span>&nbsp;¬∑&nbsp;4 min&nbsp;¬∑&nbsp;725 words&nbsp;¬∑&nbsp;Justin</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#introduction aria-label=Introduction>Introduction</a></li><li><a href=#installation aria-label=Installation>Installation</a></li><li><a href=#operations---rearrange aria-label="Operations - Rearrange">Operations - Rearrange</a><ul><li><a href=#changing-axes aria-label="Changing Axes">Changing Axes</a></li><li><a href=#flattening aria-label=Flattening>Flattening</a></li><li><a href=#unflattening aria-label=Unflattening>Unflattening</a></li></ul></li><li><a href=#operations---reduce aria-label="Operations - Reduce">Operations - Reduce</a><ul><li><a href=#mean-averaging aria-label="Mean Averaging">Mean Averaging</a></li><li><a href=#max-pooling aria-label="Max Pooling">Max Pooling</a></li></ul></li><li><a href=#operations---repeat aria-label="Operations - Repeat">Operations - Repeat</a></li><li><a href=#pytorch-layers aria-label="PyTorch Layers">PyTorch Layers</a></li><li><a href=#application---attention aria-label="Application - Attention">Application - Attention</a></li><li><a href=#references aria-label=References>References</a></li></ul></div></details></div><div class=post-content><h2 id=introduction>Introduction<a hidden class=anchor aria-hidden=true href=#introduction>#</a></h2><p><em>Einops</em> is a powerful tool for tensor operations, including <em>reduce</em>, <em>rearrange</em>, <em>repeat</em>, and more. Its most impressive feature is its ability to make code more readable and reliable. Additionally, it includes <code>nn.Module</code> functionality, which is compatible with <em>PyTorch</em> for directly modifying tensor shapes.</p><p>In this tutorial, we&rsquo;ll use the MNIST dataset for demonstration purposes.</p><p><img loading=lazy src=https://i.imgur.com/bOo6IK0.jpg alt></p><h2 id=installation>Installation<a hidden class=anchor aria-hidden=true href=#installation>#</a></h2><p>To install the stable version of <em>Einops</em>, use pip:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>pip install einops
</span></span></code></pre></div><p>For the latest version, you can install it directly from the GitHub repository:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>pip install git+https://github.com/arogozhnikov/einops
</span></span></code></pre></div><h2 id=operations---rearrange>Operations - Rearrange<a hidden class=anchor aria-hidden=true href=#operations---rearrange>#</a></h2><h3 id=changing-axes>Changing Axes<a hidden class=anchor aria-hidden=true href=#changing-axes>#</a></h3><p>The <code>rearrange</code> function allows you to change the shape of a tensor. For instance, treating an image as a matrix, you can perform a transpose operation. If the original shape of the tensor is <code>(h, w)</code>, transposing changes it to <code>(w, h)</code>. With Python, you can write:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># Shape of data: (h, w)</span>
</span></span><span style=display:flex><span>transpose_data <span style=color:#f92672>=</span> data<span style=color:#f92672>.</span>transpose(<span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>print(transpose_data<span style=color:#f92672>.</span>shape)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>&gt;&gt;</span> torch<span style=color:#f92672>.</span>Size([w, h])
</span></span></code></pre></div><p>While this code is straightforward, it doesn&rsquo;t clearly indicate the shape transformations. Using <em>Einops</em>, the same operation is more intuitive:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> einops <span style=color:#f92672>import</span> rearrange
</span></span><span style=display:flex><span>data <span style=color:#f92672>=</span> rearrange(data, <span style=color:#e6db74>&#34;h w -&gt; w h&#34;</span>)
</span></span></code></pre></div><p>Here, the transformation is described explicitly: <code>"h w -> w h"</code>. This syntax is clearer and easier to understand.</p><p><img loading=lazy src=https://i.imgur.com/7kVIwkt.png alt></p><h3 id=flattening>Flattening<a hidden class=anchor aria-hidden=true href=#flattening>#</a></h3><p>Flattening combines multiple axes into a single dimension. For example, consider a tensor with shape <code>(batch_size, width, height)</code>:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># Shape of data: (batch_size, width, height)</span>
</span></span><span style=display:flex><span><span style=color:#75715e># Shape of stack_data: (batch_size * width, height)</span>
</span></span><span style=display:flex><span>stack_data <span style=color:#f92672>=</span> rearrange(data, <span style=color:#e6db74>&#34;b h w -&gt; (b h) w&#34;</span>)
</span></span></code></pre></div><p>You can also rearrange dimensions in a non-sequential order:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>stack_data <span style=color:#f92672>=</span> rearrange(data, <span style=color:#e6db74>&#34;b h w -&gt; h (b w)&#34;</span>)
</span></span></code></pre></div><p>Output examples:</p><figure><img loading=lazy src=https://i.imgur.com/L5emdwl.png><figcaption>Combining the batch and width dimensions.</figcaption></figure><figure><img loading=lazy src=https://i.imgur.com/Qm54CBn.png><figcaption>Combining the batch and height dimensions.</figcaption></figure><h3 id=unflattening>Unflattening<a hidden class=anchor aria-hidden=true href=#unflattening>#</a></h3><p>In addition to combining axes, you can split an axis into multiple dimensions:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>decompose_data <span style=color:#f92672>=</span> rearrange(data, <span style=color:#e6db74>&#34;(b1 b2) h w -&gt; (b1 h) (b2 w)&#34;</span>, b1<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span>)
</span></span></code></pre></div><p><img loading=lazy src=https://i.imgur.com/jIKYQsP.png alt></p><p>Here, the batch size of <code>4</code> is split into two dimensions: <code>b1=2</code> and <code>b2=2</code>.</p><h2 id=operations---reduce>Operations - Reduce<a hidden class=anchor aria-hidden=true href=#operations---reduce>#</a></h2><p>The <code>reduce</code> function simplifies tensors by reducing dimensions. It shares the same syntax as <code>rearrange</code> but allows changes in dimensionality. For instance, consider a tensor with shape <code>(batch_size, height * h1, width * w1)</code>.</p><h3 id=mean-averaging>Mean Averaging<a hidden class=anchor aria-hidden=true href=#mean-averaging>#</a></h3><p>Mean averaging reduces image size by averaging pixel values:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>reduce_img <span style=color:#f92672>=</span> reduce(data, <span style=color:#e6db74>&#34;b (h h1) (w w1) -&gt; h (b w)&#34;</span>, <span style=color:#e6db74>&#34;mean&#34;</span>, h1<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span>, w1<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Check the shape</span>
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Before: </span><span style=color:#e6db74>{</span>data<span style=color:#f92672>.</span>shape<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;After: </span><span style=color:#e6db74>{</span>reduce_img<span style=color:#f92672>.</span>shape<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>&gt;&gt;&gt;</span> Before: torch<span style=color:#f92672>.</span>Size([<span style=color:#ae81ff>4</span>, <span style=color:#ae81ff>28</span>, <span style=color:#ae81ff>28</span>])
</span></span><span style=display:flex><span><span style=color:#f92672>&gt;&gt;&gt;</span> After: torch<span style=color:#f92672>.</span>Size([<span style=color:#ae81ff>14</span>, <span style=color:#ae81ff>56</span>])
</span></span></code></pre></div><p>Output:</p><p><img loading=lazy src=https://i.imgur.com/cnGjHPZ.png alt></p><h3 id=max-pooling>Max Pooling<a hidden class=anchor aria-hidden=true href=#max-pooling>#</a></h3><p>Similarly, you can apply max pooling to reduce size:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>reduce_img <span style=color:#f92672>=</span> reduce(data, <span style=color:#e6db74>&#34;b (h h1) (w w1) -&gt; h (b w)&#34;</span>, <span style=color:#e6db74>&#34;max&#34;</span>, h1<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span>, w1<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Check the shape</span>
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Before: </span><span style=color:#e6db74>{</span>data<span style=color:#f92672>.</span>shape<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;After: </span><span style=color:#e6db74>{</span>reduce_img<span style=color:#f92672>.</span>shape<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>&gt;&gt;&gt;</span> Before: torch<span style=color:#f92672>.</span>Size([<span style=color:#ae81ff>4</span>, <span style=color:#ae81ff>28</span>, <span style=color:#ae81ff>28</span>])
</span></span><span style=display:flex><span><span style=color:#f92672>&gt;&gt;&gt;</span> After: torch<span style=color:#f92672>.</span>Size([<span style=color:#ae81ff>14</span>, <span style=color:#ae81ff>56</span>])
</span></span></code></pre></div><p><img loading=lazy src=https://i.imgur.com/gsYj7e3.png alt></p><h2 id=operations---repeat>Operations - Repeat<a hidden class=anchor aria-hidden=true href=#operations---repeat>#</a></h2><p>While <code>reduce</code> decreases dimensions, <code>repeat</code> increases them. For example, to expand an image tensor from <code>(b, h, w)</code> to <code>(b, h, w, 3)</code>:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># Shape of data: (batch_size, width, height)</span>
</span></span><span style=display:flex><span>repeat_data <span style=color:#f92672>=</span> repeat(data, <span style=color:#e6db74>&#34;b h w -&gt; b h w c&#34;</span>, c<span style=color:#f92672>=</span><span style=color:#ae81ff>3</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>print(repeat_data<span style=color:#f92672>.</span>shape)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>&gt;&gt;</span> torch<span style=color:#f92672>.</span>Size([<span style=color:#ae81ff>4</span>, <span style=color:#ae81ff>28</span>, <span style=color:#ae81ff>28</span>, <span style=color:#ae81ff>3</span>])
</span></span></code></pre></div><p>Duplicating dimensions can also be done easily:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>repeat_data <span style=color:#f92672>=</span> repeat(data, <span style=color:#e6db74>&#34;b h w -&gt; (b h) (w1 w)&#34;</span>, w1<span style=color:#f92672>=</span><span style=color:#ae81ff>3</span>)
</span></span></code></pre></div><p><img loading=lazy src=https://i.imgur.com/JXlpJgj.png alt></p><h2 id=pytorch-layers>PyTorch Layers<a hidden class=anchor aria-hidden=true href=#pytorch-layers>#</a></h2><p>Einops operations can be integrated into PyTorch layers, making them ideal for use in models:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> einops.layers.torch <span style=color:#f92672>import</span> Rearrange
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>model <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Sequential(
</span></span><span style=display:flex><span>    nn<span style=color:#f92672>.</span>Conv2d(<span style=color:#f92672>...</span>),
</span></span><span style=display:flex><span>    nn<span style=color:#f92672>.</span>Conv2d(<span style=color:#f92672>...</span>),
</span></span><span style=display:flex><span>    <span style=color:#f92672>...</span>
</span></span><span style=display:flex><span>    Rearrange(<span style=color:#e6db74>&#34;b c h w -&gt; b (c h w)&#34;</span>)
</span></span><span style=display:flex><span>    <span style=color:#f92672>...</span>
</span></span><span style=display:flex><span>)
</span></span></code></pre></div><h2 id=application---attention>Application - Attention<a hidden class=anchor aria-hidden=true href=#application---attention>#</a></h2><p>Einops simplifies implementing the attention mechanism, which calculates input weights based on dot products of queries and keys. Below is an example implementation:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> torch
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> torch.nn <span style=color:#66d9ef>as</span> nn
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> einops <span style=color:#f92672>import</span> rearrange
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>Attention</span>(nn<span style=color:#f92672>.</span>Module):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> __init__(self, n_head: int, d_in: int, d_model: int, dropout<span style=color:#f92672>=</span><span style=color:#ae81ff>0.1</span>):
</span></span><span style=display:flex><span>        super()<span style=color:#f92672>.</span>__init__()
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>n_head <span style=color:#f92672>=</span> n_head
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>assert</span> d_model <span style=color:#f92672>%</span> n_head <span style=color:#f92672>==</span> <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>q <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Linear(d_in, d_model)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>k <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Linear(d_in, d_model)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>v <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Linear(d_in, d_model)
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>out <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Linear(d_model, d_in)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>dropout <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Dropout(p<span style=color:#f92672>=</span>dropout)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>layer_norm <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>LayerNorm(d_model)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self, x, mask<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>):
</span></span><span style=display:flex><span>        q <span style=color:#f92672>=</span> rearrange(self<span style=color:#f92672>.</span>q(x), <span style=color:#e6db74>&#39;b l (h q) -&gt; h b l q&#39;</span>, h<span style=color:#f92672>=</span>self<span style=color:#f92672>.</span>n_head)
</span></span><span style=display:flex><span>        k <span style=color:#f92672>=</span> rearrange(self<span style=color:#f92672>.</span>k(x), <span style=color:#e6db74>&#39;b t (h q) -&gt; h b t q&#39;</span>, h<span style=color:#f92672>=</span>self<span style=color:#f92672>.</span>n_head)
</span></span><span style=display:flex><span>        v <span style=color:#f92672>=</span> rearrange(self<span style=color:#f92672>.</span>v(x), <span style=color:#e6db74>&#39;b t (h v) -&gt; h b t v&#39;</span>, h<span style=color:#f92672>=</span>self<span style=color:#f92672>.</span>n_head)
</span></span><span style=display:flex><span>        attn <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>einsum(<span style=color:#e6db74>&#39;hblq,hbtq-&gt;hblt&#39;</span>, [q, k]) <span style=color:#f92672>/</span> q<span style=color:#f92672>.</span>shape[<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>] <span style=color:#f92672>**</span> <span style=color:#ae81ff>0.5</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> mask <span style=color:#f92672>is</span> <span style=color:#f92672>not</span> <span style=color:#66d9ef>None</span>:
</span></span><span style=display:flex><span>            attn <span style=color:#f92672>=</span> attn<span style=color:#f92672>.</span>masked_fill(mask[<span style=color:#66d9ef>None</span>], float(<span style=color:#e6db74>&#39;-inf&#39;</span>))
</span></span><span style=display:flex><span>        attn <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>softmax(attn, dim<span style=color:#f92672>=</span><span style=color:#ae81ff>3</span>)
</span></span><span style=display:flex><span>        output <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>einsum(<span style=color:#e6db74>&#39;hblt,hbtv-&gt;hblv&#39;</span>, [attn, v])
</span></span><span style=display:flex><span>        output <span style=color:#f92672>=</span> rearrange(output, <span style=color:#e6db74>&#39;h b l v -&gt; b l (h v)&#39;</span>)
</span></span><span style=display:flex><span>        output <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>dropout(self<span style=color:#f92672>.</span>out(output))
</span></span><span style=display:flex><span>        output <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>layer_norm(output <span style=color:#f92672>+</span> x)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> output
</span></span></code></pre></div><h2 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h2><ol><li><a href=https://einops.rocks>Einops Documentation</a></li></ol></div><footer class=post-footer><ul class=post-tags></ul><nav class=paginav><a class=prev href=https://justin900429.github.io/en/posts/paper_reading/dit-intro/><span class=title>¬´ Prev</span><br><span>Diffusion Transformer: A powerful archiecture for image generation</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2024 <a href=https://justin900429.github.io/en/>Justin's Site</a></span> ¬∑
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>